\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{subcaption} % For subfigures
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\usepackage[textsize=tiny]{todonotes}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{gensymb}
\usepackage{enumitem}

\bibliographystyle{IEEEtran}

\graphicspath{{figures}}

\begin{document}

\title{A Robust 3D Grid-Based Hybrid Approach for Ground Segmentation in Point Clouds Across Varied Terrains
\thanks{Funded via some awesome projects}
}

\author{\IEEEauthorblockN{Muhammad Haider Khan Lodhi}
\IEEEauthorblockA{\textit{Robotics Innovation Center}\\
German Research Center for Artificial Intelligence (DFKI)\\
Bremen, Germany\\
mulo01@dfki.de}%
\and
\IEEEauthorblockN{Christoph Hertzberg}
\IEEEauthorblockA{\textit{Robotics Innovation Center}\\
German Research Center for Artificial Intelligence (DFKI)\\
Bremen, Germany\\
christoph.hertzberg@dfki.de}%
}%

\maketitle

\begin{abstract}
Ground segmentation in point cloud data is the process of separating the points
into ground and non-ground. It is a key component in the perception pipeline of
an autonomous system. Many solutions have been published in literature but
they are targeted to solve the ground segmentation challenge for autonomous
driving and are based on the assumption that the ground surface is composed of
flat surfaces. There are very few solutions which are capable of performing in
indoor and outdoor environments with various degrees of terrain roughness. Additionally,
the existing solutions usually require hand tuning of many parameters for
optimal performance and are specific for certain type of LIDAR sensors and
sensor configurations. There is a need to have a general purpose, easy to use,
all-terrain capable ground segmentation solution which does not require
extensive tuning of parameters. In this work, we provide such a solution for
rough terrain indoor and outdoor mobile robotics and autonomous driving applications. Our solution
combines the strengths of different approaches as a single solution and produces
good ground segmentation on varied terrains.
\end{abstract}

\begin{IEEEkeywords}
LIDAR, Ground segmentation, Mobile Robotics, Rough Terrain, Autonomous Driving
\end{IEEEkeywords}

\section{Introduction}
The perception system of an autonomous system utilizes multiple sensors to
capture salient features of the environment. These features are used as inputs by various sub
systems e.g. navigation, control, learning etc. A core sensor of the perception system
is the LIDAR and given the advancements in sensor design and reduced costs, it
has become a standard sensor in autonomous systems \cite{todo}. The LIDAR uses bands of
laser beams to scan the surrounding environment and the time-of-flight
information is used to capture distance information at high resolution. The
result of the single scan is a point cloud sample which in today’s sensors may
contain more than 1~million points. Evidently, the point cloud captures the
points from the environment in the scanning region of interest. However, not all 
points in the region of interest are relevant for the downstream algorithms. For example, the obstacle detection 
algorithm is only interested in the obstacle points and the road detection algorithm only requires the ground points. 
Feeding the same point cloud sample to both algorithms would require redudant processing of irrelevant points in both 
algorithms. To mitigate the redudancy, it is a usual practice to segment the points into ground and non-ground points. 
The segmentation task is performed by a ground segmentation algorithm and it has many applications:
\begin{itemize}
  \item Ground Detection
  \item Obstacle Detection
  \item Object Detection
  \item Terrain Analysis
  \item Traversability Analysis
  \item Drivable Area Detection
  \item SLAM
\end{itemize}

\begin{figure*}
  \centering
  \includegraphics[width=\linewidth]{intro.png}
  \caption{Results of the ground segmentation pipeline
  \todo[inline]{Split into subfigures. Move this or some figure into intro-section}}
  \label{fig:results}
\end{figure*}

Ground point extraction from a point cloud sample is a crucial step in the perception pipeline of autonomous systems. 
Numerous solutions have been proposed over the past few years. In this work, our primary focus was on extracting ground 
points with minimal parameter dependency. We aimed to eliminate the need for parameter fine-tuning, ensuring 
adaptability across various vehicles, robots, sensor configurations, and both indoor and outdoor environments. 
Our segmentation algorithm was tested using a consistent set of parameters on real-world datasets, featuring different 
types of point cloud sensors in various configurations and environments. The parameters in our solution are intuitive, 
easy to understand, and do not require advanced domain knowledge. The segmentation process combines local eigen analysis, 
model fitting, and region-growing algorithms. We explore the strengths and weaknesses of these approaches and integrate 
them into a processing pipeline to achieve effective ground segmentation in diverse indoor and outdoor scenarios.

\subsection{Statement of Need}
\begin{itemize}
  \item Detection of ground points is a necessary precursor step for extraction
    of useful information from point cloud data for downstream tasks in mobile
    robot navigation \cite{Jimenez2021,Arora2023}.
  \item Object and obstacle detection algorithms detect ground points as false
    positives. The detection can be improved when ground points are detected and
    removed. Additionally, the computational burden is reduced by processing only
    non-ground points \cite{Firkat2023}.
  \item Ground points can be used for traversability analysis, navigation, and
    static map generation.
  \item A solution is needed which is independent of parameter tuning, sensor configuration,
  and to an extend the type of environment.
\end{itemize}


\section{Related Work}
Existing ground segmentation methods typically rely on either traditional techniques or learning-based approaches. 
This classification is supported by a recent survey on ground segmentation methods \cite{Gomes2023}, which 
categorizes the current state-of-the-art solutions accordingly.

\subsection{Traditional Solutions}
The traditional solutions for ground segmentation use geometric properties of the
points to segment the ground and non-ground points. Usual approaches discretize
the point cloud into grid cells and then process the points in each cell
individually. The grid cells may be purely 2D, 2.5D, or even 3D \cite{todo}. Some
approaches extend the grid cell and analyze the points in neighboring cells for
a better estimation of surface properties \cite{todo}. The modeling of the points in the
grid cells using  plane fitting and line extraction is a common approach in many
solutions \cite{todo}. Region growing and clustering is used to combine individual grid
cells into a larger region of connected cells \cite{todo}. Higher order inferences based
on Conditional Random Field (CRF) and Markov Random Field (MRF) have been used
\cite{todo}.

\subsection{Learning Solutions}
The advancements in deep learning meant that some researchers used the existing
CNN networks originally used for object detection for ground segmentation using
range images generated from the point cloud data.
The development of PointNet \cite{qi2017pointnet} and its successor PointNet++ \cite{qi2017pointnet++} give a unified
solution for various applications e.g. object and scene segmentation. Various
other solutions like GndNet \cite{paigwar2020gndnet}, PointPillars \cite{lang2019pointpillars}, JCP \cite{todo}, SectorGSnet \cite{todo}
showcase good results.

\section{Proposed Solution}
When selecting a ground segmentation algorithm, it is crucial to understand their
most significant differences and features that best suit the requirements of the
ﬁnal application. Our goal was to develop a ground segmentation solution based
on the traditional approaches. After analyzing the state-of-the-art, we decided
to combine different approaches as a single solution by utilizing the best of
all approach.

\subsection{Key Challenges in the Existing Traditional Solutions}
We noticed some key shortfalls in the existing solutions and aimed to come up with a solution which can solve the open challenges.
Below we list some of the key challenges. The challenges were found out after careful experimentation of various datasets and sensor 
configurations. We documented the challenges in the experimentaion section at the end of the paper.

The key challenges faced when working with the existing state-of-the-art solutions are that they;
\begin{itemize}
  \item require fine tuning of many algorithm parameters for best performance.
  \item have high degree of performance dependency on the type of sensor used for the point cloud.
  \item are developed for the autonomous driving scenario with only a few solutions showcasing adequate performance
        of rough terrains.
  \item show good performance on the SemanticKITTI datset but do not adapt to other datasets from the autonomous driving domain.
\end{itemize}

Below we discuss some some key challenges which researches try to solve in the
ground segmentation approaches and suggest our selected solution for it.

\subsection{Computational Requirements}
A high-resolution LiDAR sensor can generate millions of data points. For
instance, the Velodyne VLS-128 can produce up to 9.6 M points per second in the
dual-return mode, with frame rates varying from 5 Hz to 10 Hz. In a typical
operation, this sensor can be configured to produce, on average, a point cloud
of 2,403,840 points per second (240,384 points at 10 Hz), which makes it harder
to analyze the entire point cloud in real time during the navigation tasks.

\paragraph*{Solution} We use a grid-based technique in 3D space to help mitigate these problems, 
where each grid cell contain its assigned points and can be processed individually on demand.

\subsection{Terrain Roughness}
The assumption of flat ground surfaces may be plausible in a urban environment but it 
does not always hold for the mobile robots. Moble robots usually have to navigate 
on uneven terrain with varying degrees of roughness. Therefore, the ground segementation algorithms
are not always easily applicable for mobile robots.

\paragraph*{Solution} We use a combination of local and global terrain properties e.g. surface gradiant,
local spacial distribution of points, and global connectivity. We apply local eigen analysis to extract local 
terrain surface properties and utilize a connectivity-based region growing algorithm based on initial seed selection. 

\subsection{Size of Grid Cell}
We faced the challenge regarding the selection of a suitable grid cell size in 
relation to the roughness of the terrain. In principle, small grid cells are better 
suited for model fitting but the grid cell's size has a direct impact on the computational performance
of the algorithm. The selected size also depends on the gap of the scan lines.
The scan lines in close proximity to the robot are dense and the distance
between the lines increases based on the distance from the robot. Irrespective
of the grid cell size, a cell close to the robot is more likely to have points
from multiple scan lines assigned to it. This means that such cells are better
suited to fit a plane model. As a consequence, it is possible to have an
accurate slope estimate of the local terrain surface. On the contrary, cells which
are assigned points far away from the robot are more likely to have only single
scan lines assigned to them. A plane fitting algorithm can fit a plane to the
line of points but the slope estimate is highly uncertain and is usually
unreliable. Additionally to this, a small cell size is more sensitive to noisy 
sensor data.

Furthermore, the size of the grid cells is also closely related to the type of
application and environment. In indoor environments with flat surfaces, it is
generally unproblamatic to have large grid cell size as compared to outdoor
uneven terrain. To sum up, there is so one size fits all solution when it comes
to selection of a suitable grid cell size for a ground segmentation algorithm.

\paragraph*{Solution} We make use a phase-wise approach and vary the size of grid
cell. In the first phase, we use a large height for the grid cells to capture as 
many tall structures as possible. The second phase is applied on the resultant 
ground points of the first phase and we use a small height for the grid cells to 
remove any false positive ground points.

\subsection{Point Sparsity}
As mentioned earlier, the point assignment to grid cells is non uniform and
depends on various factors. It is not feasible to blindly fit a plane to all
grid cells because some cells may have a single line of points or a random
distribution of points. This is especially the case when using a fixed grid size
and the distance of the cell from the point cloud origin increases.

\paragraph*{Solution} We perform local eigen analysis on each grid cell. The eigen values
encode a good representation of the spacial distribution of the points assigned to the cell. 
Afterwards, we classify each grid cell into one of the two classes: Line and Plane. Each 
cell type is handed with a different technique most appropritate for it. We explain the
details in the section X.Y.

\subsection{Polar \& Square Coordinates}
Literature review has shown that researchers prefer polar grid cells as compared
to square cells. Various types of polar grid cells have been researched.
\paragraph*{Solution} However, in our experiments we found a square grid cell
with a fixed grid size performs better than polar cells. The reason in our view
is that as the distance from the sensor origin increases, the cells get bigger
and as a concequence, results in the deterioration of the performance of the
plane fitting based segmentation.

\subsection{False positive flat surfaces e.g. table top, car roof etc.}
The gradient of the fitting plane to a grid cell can not be used as the sole
criteria for the segmentation. The reason is the existence of flat surfaced
obstacles in the robot's environment.
\paragraph*{Solution} We apply region growing based on high confidence intial
ground seed cells. The connectivity-based expansion ensures that only
neighboring ground cells are expanded.

\subsection{False positive points at the junction of ground and non-ground points}
The results of the segmentation depend on the handling of junctions between
ground and non-ground points. This is where the majority of cases of under and over
segmentation occur. 

\paragraph*{Solution} A point-wise inlier check is used at the
junction of ground and obstacle cells to detect and correct over and under segmentation.

\subsection{Influence of grid cell height on the performance of segmentation}
The height of the grid cell plays a key role in identification of obstacles in
the point cloud. The key challenge here is to detect tall obstacles in the grid
cells but without false detection the underlying ground points as obstacles.
This challenge is evident in the tree canopy situation where the ground points
in the grid cell are falsely identified as obstacle points because of the large
height of the grid cell.
\paragraph*{Solution} The point space is sub-divided into 3D grid cells for a clear distinction
between ground and non-ground points. Additionally, we use the same strategy as mentioned in the 
point G to detect and correct segmentation mistakes.

\subsection{Fitted Plane Outlier Correction}
The real world ground surface has a curvature and plane fitting usually results in false 
positive outliers upon fitting a ideal flat surface to the the points. 
\paragraph*{Solution} It is common practice to define a distance threshold to fit a planar model to a set of 
points. We also use a fixed distance threshold for the initial plane fit for for gradient estimation. However, the 
extraction of ground and non-ground points uses an additional correction step using the actual resultant normal of 
the points assinged to the cell. We use the strategy as mentioned in the point G to detect and correct segmentation mistakes.

\subsection{Performance}
The requirement for real-time operation of the ground segmentation is very
evident for the application in the autonomous driving community. Therefore,
solutions need to able to do a highly accurate segmentation within in stipulated
time period. The exact time requirements may vary based on the industry.

\paragraph*{Solution} \todo{Time and space complexity analysis needs to be done}

\section{Core Components}

\subsection{Data Preparation and Filtering}
The input point cloud is cropped to a user-defined region of interest, which varies depending on the application. 
For autonomous driving, a larger region of interest is preferred, while mobile robotics typically require a smaller
to medium-sized region. To manage the number of points, we downsample the point cloud to a resolution of 10 cm. 
The downsampling and the respective resolution is determined by specific algorithm parameters.

\section{Two-Phase Segmentation of Points Based on Local Surface Properties and Neighborhood Analysis}

The segmentation process is divided into two distinct phases to address different challenges in point classification. 
Each phase uses grid cells of varying heights to improve the accuracy of ground and obstacle detection, 
while also managing potential misclassifications.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.5\textwidth]{figures/overview/stages.png}
  \caption{Stages}
  \label{fig:pipeline}
\end{figure}

\textbf{First Phase: Large Grid Cell Height}: In the first phase of segmentation, a large height for the grid cells in the \( z \)-direction is employed. 
This approach is designed to capture as many obstacles as possible within each grid cell. The larger cell size helps 
in identifying obstacles that might otherwise be missed.

However, this method comes with a trade-off. The increased cell height can lead to the misclassification of ground points, 
especially in scenarios where the ground is obscured by overhead objects, such as tree canopies. As a result, some ground 
points may be incorrectly labeled as non-ground. These false detections are addressed in subsequent stages of the segmentation process.

\textbf{Second Phase: Small Grid Cell Height}: The second phase of segmentation uses a smaller height for the grid cells. This smaller cell size is crucial for 
refining the segmentation results by minimizing false positives identified as ground points in the first phase. 
The reduced cell height allows for more precise detection of ground and obstacle points.

During the post-processing phase, any points that were incorrectly classified as non-ground in the first phase but 
are actually ground are reclassified and added back to the ground points. This correction ensures that the final 
classification is accurate, reflecting the true distribution of ground and non-ground points.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.5\textwidth]{figures/overview/pipeline.png}
  \caption{Pipeline}
  \label{fig:pipeline}
\end{figure}

The steps involved in the segmentation process are:

\begin{itemize}
  \item Grid Representation
  \item Local Eigen Analysis (LEA)
  \item Surface Gradient Analysis (SGA)
  \item Ground Region Growth (GRG)
  \item Final Ground Segmentation (FGS)
\end{itemize}

The behavior of these steps varies between the first and second phases. The table below summarizes the changes in behavior for each step during these phases.

\begin{table}[h!]
  \centering
  \caption{Behavior of steps in the first and second phases of segmentation}
  \resizebox{\columnwidth}{!}{%
  \begin{tabular}{|p{4cm}|p{2cm}|p{2cm}|}
  \hline
  \textbf{Step} & \textbf{First Phase} & \textbf{Second Phase} \\
  \hline
  \textbf{Grid Representation} & Grid cells have a large height & Grid cells have a small height \\
  \hline
  \hline
  \textbf{Local Eigen Analysis} & Unchanged & Unchanged \\
  \hline
  \textbf{Surface Gradient Analysis} & Unchanged & Unchanged \\
  \hline
  \textbf{Ground Region Growth} & Unchanged & A detailed neighborhood check is performed for neighboring cells during expansion \\
  \hline
  \textbf{Final Ground Segmentation} & Unchanged & Unchanged \\
  \hline
  \end{tabular}%
  }
  \label{tab:phase_behavior}
\end{table}

\subsection{Grid Representation 3D}
The points are assigned to cells within a 3D grid in Euclidean space based on their positions. This 3D grid is crucial 
for modeling and detecting empty spaces, as the height of each grid cell helps identify voids or gaps in the environment. 
The dimensions of the grid cells in the x, y, and z axes are determined by three specific algorithm parameters, ensuring 
that the grid accurately represents the spatial characteristics of the point cloud.
\begin{align}
cell_x &= point_x / cellsize_x \\
cell_y &= point_y / cellsize_y \\
cell_z &= point_z / cellsize_z
\end{align}
where $cellsize$ is a parameter. We explain our choice of parameters in the section H.

\subsection{Local Eigen Analysis}
In point cloud data, the density of points often decreases as the distance from the sensor increases. 
This relationship arises because most sensors, such as LiDAR or depth cameras, have a limited resolution 
and a wider field of view at greater distances. Consequently, the farther the points are from the sensor, 
the fewer points are collected per unit area, leading to increased sparsity.

To address this issue in point cloud processing, especially when fitting geometric models, we use eigenvalue 
analysis to estimate the local spatial distribution of points within each grid cell. The eigenvalue analysis 
helps determine how well a grid cell approximates a planar surface, which is challenging when point density is low.

Given a point cloud within a grid cell, we can calculate the eigenvalues of the covariance matrix of the points 
to assess their spatial distribution. Let CC be the covariance matrix computed for the points in a grid cell. 
The eigenvalue decomposition of CC yields eigenvalues $\lambda_1$, $\lambda_2$, and $\lambda_3$ (where $\lambda_1$ > $\lambda_2$ > $\lambda_3$).

We use the ratio of the largest eigenvalue to the sum of all eigenvalues to gauge the distribution:

\begin{align}
  \text{Ratio} &= \frac{\lambda_1}{\lambda_1 + \lambda_2 + \lambda_3}
\end{align}  

The ratio of the largest eigenvalue to the sum of all eigenvalues provides an indication of whether the points in 
a grid cell are more aligned with a planar surface, a line, or a non-planar structure. The ratio is calculated as follows:

We devise three possible conclusions based on the ratio and categorize the grid
cells into three categories.

\begin{description}[style=nextline]
  \item[Line Cell]
  A high ratio in range $[0.9, 1.0]$ indicates that $\lambda_1$ is significantly larger than $\lambda_2$ and $\lambda_3$, which suggests that the points 
  are distributed along a line. In this case, the distribution is highly elongated along one direction with minimal variation 
  in the other directions.
%
  \item[Plane Cell]
  A ratio in this range $[0.4, 0.9)$ indicates that $\lambda_1$ is moderately larger than $\lambda_2$ and $\lambda_3$, suggesting that the points 
  are more aligned with a planar surfaceFinal Ground Segmentation
%
  \item[Non-Plane Cell]
  A ratio in this range $[0.4, 0.9)$ indicates that $\lambda_1$ is moderately larger than $\lambda_2$ and $\lambda_3$, suggesting that the points 
  are more aligned with a planar surfaceFinal Ground Segmentation
\end{description}

\begin{table}
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Cell Type} & \textbf{Threshold} \\
\midrule
Line Cell      & $> 0.95$ \\
Plane Cell     & $\ge 0.40$ \\
Non-Plane Cell   & $< 0.40$ \\
\bottomrule
\end{tabular}
\caption{Largest eigen to the sum ratio}
\label{tab:eigen-ratio}
\end{table}

Each type of cell is processed using a distinct algorithm, and we refer to these cells as Line, Plane, and Non-Planar cells:

\subsubsection{Line Cell}
\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{line_cell.png}
  \caption{Points (white) assigned to a grid cell and the 
  corresponding eigen vectors (green) are shown. 
  The eigen vector along the line of points is the single dominant eigen value. 
  The cell is classified as Line based of the local eigen analysis.}
  \label{fig:LineCell}
\end{figure}

If the largest eigenvalue is significantly larger than the other eigenvalues, the cell is categorized as a Line Cell.
In such cases, the cell exhibits a dominant linear structure. For these cells, we compute the angle between the largest eigenvector 
(depicted in green in Fig.~\ref{fig:LineCell}) and the positive z-axis (pointing upwards). This angle helps to characterize 
the orientation of the linear distribution relative to the vertical axis.

\begin{align}
CellType &= 
  \begin{cases}
    \text{Tentative Obstacle} & angle \ge 90\degree - Threshold \\
    \text{Tentative Ground} & angle < 90\degree - Threshold
  \end{cases}
\end{align}

The ground slope threshold is used to determine the classification of cells based on their angle of uprightness. 
Cells with a small angle between the largest eigenvector and the positive z-axis are considered for classification
as obstacles. This is because ground cells typically do not exhibit a highly upright spatial distribution.

When the angle of the largest eigenvector is small, indicating that the points are not upright, the cell might be 
categorized as either an Obstacle or Ground. However, because this configuration could also represent the top of an
obstacle (such as a car roof or a table), we cannot be certain of its exact nature. As a result, such cells are marked
as tentative Obstacle until further analysis can confirm their true classification.

\subsubsection{Plane Cell}
\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{plane_cell.png}
  \caption{Points (white) assigned to a grid cell and the corresponding eigen vectors (green) are shown. 
  Two dominant eigen vectors and shown and the third vector is negligible in comparison. 
  The cell is classified as Plane based on the local eigen analysis.}
  \label{fig:PlaneCell}
\end{figure}
Cells that contain multiple scan lines are suitable candidates for plane fitting when the spatial distribution of the points 
exhibits two dominant eigenvalues, with the third eigenvalue being negligibly small or significantly smaller. In such cases, 
the cell is assessed to determine if it can be classified as a Plane Cell.

Specifically, if the ratio of the largest eigenvalue to the sum of all eigenvalues is greater than 0.4, the points are 
classified as Plane. This ratio indicates that the points align well with a planar surface, where the largest eigenvalue 
reflects the direction perpendicular to the plane, and the other two eigenvalues are relatively small, suggesting minimal 
variation within the plane.
\todo{Suggestion: One could check lowest EV/EV sum $<$ threshold}

\subsubsection{Non-Plane Cell}
\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{unknown_cell.png}
  \caption{Points (white) assigned to a grid cell and the corresponding eigen vectors (green) are shown. 
  There is no dominance of a single or two eigen vectors. 
  The cell is classified as Unknown based on the local eigen analysis.}
  \label{fig:UnknownCell}
\end{figure}

If the ratio of the largest eigenvalue to the sum of all eigenvalues is smaller than 0.4, the points are 
classified as Obstacle. In this scenario, the cell does not exhibit two clearly dominant eigenvalues, 
suggesting that the points do not align well with a planar surface.

While the threshold of 0.4 helps in distinguishing between Plane and Obstacle cells, it may lead to some false 
classifications. Specifically, cells that should be classified as Plane or Unknown might be incorrectly 
categorized due to the sharp cutoff. However, these misclassifications are addressed and corrected automatically 
during subsequent stages of the processing pipeline. Each Obstacle cell is individually processed in the segmentation stage.

\subsection{Planar Model Fitting}
\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{plane_model_fit.png}
  \caption{Plane model fit for local surface gradient estimation.}
  \label{fig:PlaneFit}
\end{figure}

We chose a model-fitting approach for our solution because, when the input point cloud is divided into smaller cells, 
it is reasonable to assume that these local cells are flat, given the small cell size. This assumption holds true more 
often than not due to the reduced cell size, which leads to more accurate plane fitting. However, this approach increases 
computational costs due to the larger number of cells.

A larger grid cell size improves performance and yields good results in indoor environments or autonomous driving scenarios. 
Nonetheless, it tends to flatten the natural curvature of the ground in outdoor settings. In areas with highly uneven terrain, 
using a large grid size can lead to poor results, as the planar model fitting might ignore some points. This raises the question 
of how to handle points marked as outliers but which are not truly outliers.

Another challenge with the grid representation is the variability in point distribution within a cell. Depending on point 
sparsity, a single cell may contain a line of points, a few lines, or even scattered points. This variability complicates 
the grid-based point assignment.

The plane model fitting is applied specifically to cells categorized as Plane. For these cells, we compute the gradient of 
the fitted plane and apply a slope threshold to assess whether the cell meets the ground criteria. Cells that fulfill these 
criteria are tentatively marked as potential ground cells. These ground cells are then used as initial seeds for the 
region-growing algorithm. Cells that do not meet the ground criteria are marked as non-ground.

\begin{align}
CellType &= 
  \begin{cases}
    \text{Ground} & angle \le Threshold \\
    \text{Obstacle} & angle < Threshold
  \end{cases}
\end{align}
\todo{One comparison needs to be turned}
where threshold refers to the ground slope threshold.

\subsection{Seed Selection}
The seed cells are selected from the tentative Ground cells based on the following conditions:

\begin{enumerate}
  \item \textbf{High Confidence of Ground Classification:} The cell is considered to have high confidence if more than 95 percent 
  of the points within it are inliers of the planar model fit.
  
  \item \textbf{Proximity to Sensor:} The centroid of the cell must be within a user-defined distance threshold. Cells that are far 
  from the sensor typically have sparse points with large gaps, making them unsuitable for initial seed selection due to limited 
  connectivity with neighboring cells. Therefore, a distance limit is set to ensure that only cells close to the sensor are considered 
  as potential seed cells.
  
  \item \textbf{Quadrant-Based Seed Selection:} Seed cells are selected from each of the four quadrants in the Euclidean space. 
  This approach helps mitigate issues related to large occlusions or gaps in the point cloud, which could inhibit region growth 
  if only a single seed cell were used. By selecting seed cells from four distinct groups (one per quadrant), we improve the 
  coverage and connectivity of the region-growing process.
  
  \item \textbf{Mean Height Calculation:} The mean height of the selected cells in each group is computed. This step helps 
  minimize the impact of any false positive seed cells. A group is considered valid only if it contains at least a minimum 
  number of selected seed cells, ensuring a robust basis for the region-growing process.
  
  \item \textbf{Selection of a Single Seed Cell:} From each valid group, one seed cell is selected based on the following criteria:
  \begin{itemize}
  \item The cell must have the highest number of ground neighbors compared to other cells in the group.
  \item It should be closest to the mean height of the group.
  \end{itemize}
  
  \item \textbf{Final Seed Cells:} After the selection process, one seed cell is chosen from each of the four quadrants.
  \end{enumerate}

\subsection{Ground Region Growth}
We recursively expand all ground neighbors of the seed cells. This region-wise growth process ensures that cells falsely 
classified as ground cells (i.e., cells that meet the slope criteria but are not truly ground) are not included in the final result.

When expanding the neighbors of a cell, we consider up to 24 neighboring cells in its vicinity. To avoid including 
distant or irrelevant neighbors, we apply a maximum distance threshold. Empty cells are not stored in the 3D grid, 
which helps in focusing only on relevant neighboring cells.

Each ground neighbor within the distance threshold is expanded and subsequently marked as expanded. This designation 
prevents any cell from being expanded more than once, ensuring efficiency and accuracy in the expansion process.

\subsection{Final Ground Segmentation}

The final ground segmentation is a critical step in the pipeline for accurately interpreting ground points from the input point cloud data, 
especially at junctions of ground and obstacle cells. This process involves analyzing and classifying the points in the cells based on
their surface and neightborhood properties. The goal is to refine the cells obtained from previous steps in the pipeline and ensure 
that each point in the cells is accurately categorized, considering both its local properties and its relationship with neighboring cells.

\subsubsection{Ground Points}

In our approach to point segmentation, each ground cell obtained from the region-growing phase is processed individually. 
This helps us determine whether any outliers from the plane fitting were incorrectly marked. Specifically, we use the smallest 
eigenvalue of the points in the cell, rather than the normal to the plane fit, to check for false negatives. This pointwise check 
helps in identifying and correcting false outliers.

The process involves the following steps:

\paragraph*{Outlier Check}

\begin{enumerate}
\item Identify the closest inlier point and compute the vector from this inlier to the outlier.
\item Calculate the component of this vector along the ground normal.
\item Based on the distance along the normal:
\begin{enumerate}
\item If the distance is less than or equal to the plane fit threshold, the outlier is classified as a ground point.
\item If the distance exceeds the plane fit threshold, the outlier is classified as a non-ground point.
\end{enumerate}
\end{enumerate}

\subsubsection{Non-Ground Cells}

Segmentation of non-ground cells presents additional challenges, especially at the boundaries where obstacles meet ground surfaces. 
To address these challenges, we perform the following checks:

\begin{enumerate}
\item Neighbor Check: We first check if a non-ground cell has any ground neighbors. This step is crucial because false segmentation 
often occurs at the junctions between obstacles and ground cells.
\begin{itemize}
\item If the non-ground cell has no ground neighbors, all its points are classified as non-ground.
\item If the non-ground cell does have ground neighbors, we need to perform further analysis to determine the validity of these neighbors.
\end{itemize}
\item Pointwise Check: For non-ground cells with ground neighbors, we perform a pointwise comparison. This involves analyzing the 
obstacle points in the cell and comparing them with neighboring ground points. The goal is to identify and remove any false positive obstacle points.
\end{enumerate}

By implementing these procedures, we refine the segmentation of non-ground cells and improve the overall accuracy of the point 
classification. This approach helps in reducing false positives and ensuring that the point cloud data accurately reflects the 
true nature of the environment.

\section{Experiments}

The experimentation was performed exclusively on datasets with real world point cloud data. We did not limit our experiementation merely to the SemanticKITTI
dataset. We selected a diverse set of datasets from urban and natural environments. Additonally, the datasets use a different sensor type of sensor and configuration.
n ground segmentation solution must be able to perform robustly for various sensor types and configurations without parameter tuning. To showcase that our solution is not
bound to a sensor height, we selected a handheld dataset to test the adaptability of our ground segmentation to a changing height of the sensor. In contrast, most existing 
solutions require a sensor height as a parameter with the assumption that the height of the sensor is unchanged. This assumption does not hold in case of handheld or UAV 
based point cloud datasets. Futhermore, the constant sensor height assumption does not hold for mobile robots with the capability to change their height.

\subsection{Performance Metrics}

We use precision, recall and F1 score as a metrics to compare the algorithms. Not all datasets provided labelled point clouds, therefore the performance metrics were 
collected only on the SemanticKITTI dataset. Visual analysis of ground segmentation results on other datasets was enough to compare the performance of GroundGrid3D and
state-of-the-art solutions.

\begin{equation}
\text{Precision} = \frac{\text{NTP}}{\text{NTP} + \text{NFP}}the proposed
\end{equation}

\begin{equation}
\text{Recall} = \frac{\text{NTP}}{\text{NTP} + \text{NFN}}
\end{equation}

\begin{equation}
\text{F1 Score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
\end{equation}

Where:
\begin{enumerate}
  \item NTP = Number of True Positives (correctly predicted positives)
  \item NFP = Number of False Positives (incorrectly predicted positives)
  \item NFN = Number of False Negatives (positives that were incorrectly predicted as negative) 
\end{enumerate}

\subsection{Parameters}
One of our main objectives was to develop and algorithm which does not require parameter tuning. We achieved our target and
all the experimantal results from various datasets use a single set of parameters. 

\begin{table}[h!]
  \centering
  \caption{Parameters of GroundGrid3D}
  \resizebox{\columnwidth}{!}{%
  \begin{tabular}{|p{4cm}|p{2cm}|p{4cm}|}
  \hline
  \textbf{Parameter} & \textbf{Value} & \textbf{Description} \\
  \hline
  \textbf{(maxX, minX, maxY, minY, maxZ, minZ)} & (80, -80, 80, -80, 80, -80) & Region of interest (in meters) of the point cloud in Euclidean space. \\ 
  \hline
  \textbf{downsample} & False & Toggle to downsample the point cloud to a user-defined resolution in meters. \\ 
  \hline
  \textbf{downsample\_resolution} & 0.1 & Point cloud is downsampled to 10 cm. \\ 
  \hline
  \textbf{(cellSizeX, cellSizeY, cellSizeZ)} & (2, 2, 10) & Size (in meters) of the grid cell in three-dimensional space. \\ 
  \hline
  \textbf{startCellDistanceThreshold} & 10 & Maximum distance for selection of seed cells used as start cells for region growing. \\
  \hline
  \textbf{groundInlierThreshold} & 0.1 & Inlier threshold (in meters) for the plane model fitting to points in a grid cell. \\ 
  \hline
  \end{tabular}%
  }
  \label{tab:parameters}
\end{table}

\subsection{Datasets}
We used a varied set of available datasets to test the adaptability of GroundGrid3D to new sensors and environments. The datasets provide data from
urban and natural environments. Each dataset uses a different type of sensors. The platforms used for data collection are Car, Mobile Robot and Handheld.

\begin{table}[h!]
  \centering
  \caption{Datasets from natural and urban environments for experiementation}
  \resizebox{\columnwidth}{!}{%
  \begin{tabular}{|p{2cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|}
  \hline
  \textbf{Dataset} & \textbf{Environment} & \textbf{Sensor} & \textbf{Year} & \textbf{Platform} & \textbf{Sensor Height (m)}\\
  \hline
  \textbf{SemanticKITTI \cite{todo}} & Urban & Velodyne HDL-64E & 2013 & Car & 1.723 \\ 
  \hline
  \textbf{UrbanNav \cite{todo}} & Urban & Velodyne HDL-32E & 2021 & Car & 2.1 \\ 
  \hline
  \textbf{DOALS \cite{todo}} & Urban & Ouster OS0 128 & 2021 & Handheld & 2 \\ 
  \hline
  \textbf{RELLIS-3D \cite{todo}} & Natural & Ouster OS1 64 & 2022 & Mobile Robot & 1.16 \\
  \hline
  \textbf{BotanicGarden \cite{todo}} & Natural  & Velodyne VLP-16 & 2024 & Mobile Robot & 1.164 \\
  \hline
  \end{tabular}%
  }
  \label{tab:phase_behavior}
  \end{table}

\subsubsection{SemanticKITTI}
The SemanticKITTI dataset \cite{Geiger2013IJRR} is a comprehensive collection of real-world data designed to
facilitate research in autonomous driving and computer vision. Collected by the
Karlsruhe Institute of Technology and the Toyota Technological Institute at
Chicago, it includes data captured from a vehicle equipped with various sensors,
such as high-resolution stereo cameras, 3D laser scanners, and GPS/IMU systems.
This dataset covers a wide range of urban, suburban, and rural environments,
providing benchmarks for tasks such as object detection, tracking, segmentation,
and visual odometry. Its diversity and complexity make it a pivotal resource for
advancing autonomous driving technologies and robust computer vision algorithms.

\subsubsection{UrbanNav}
The UrbanNav dataset, available on GitHub, is a comprehensive dataset designed for urban navigation research, 
particularly in Global Navigation Satellite System (GNSS)-denied environments. Developed by the Intelligent 
Positioning and Navigation Laboratory at The Hong Kong Polytechnic University, it provides data collected 
from various sensors, including GNSS receivers, Inertial Measurement Units (IMUs), Light Detection and Ranging (LiDAR) sensors, 
and cameras, in dense urban settings like Hong Kong. The dataset is intended to facilitate the development and testing of robust 
navigation algorithms that can handle the challenges of urban environments, such as multipath effects and signal blockages. The 
repository includes detailed descriptions of the data collection process, file structure, and usage guidelines, making it a valuable 
resource for researchers working on urban navigation and related fields.

\subsubsection{DOALS}
Dynamic Object Aware LiDAR SLAM based on Automatic Generation of Training Data (DOALS) contains 12000 scans that were recorded
in the main hall of ETH Zurich. The recording was performed at various locations and each location provides 2 sequences of data.
We use the data collection from the touristic pedestrian zone (Nierendorf) for our ground segmentation tests.

\subsubsection{BotanicGarden}
The BotanicGarden dataset is a used to test the ground segmentation capabilities of the algorithm in unstructured natual environments. The
dataset provides data from wide varierty of sensors, including high-res and high-rate stereo gray and RGB cameras, 
spinning and MEMS 3D LiDARs, and low-cost and industrial-grade IMUs, supporting a wide range of applications. 

\subsubsection{RELLIS-3D \cite{todo}}
The Rellis-3D dataset, developed by Texas A\&M University, is an advanced dataset
aimed at improving semantic segmentation in outdoor environments. Named after
the RELLIS Campus, it features high-resolution LiDAR and camera data captured in
diverse weather conditions and various terrain types. Unlike many other datasets
focused on urban settings, Rellis-3D emphasizes off-road scenarios, including
complex and cluttered scenes with vegetation, rough terrain, and obstacles. This
dataset is instrumental for developing and testing algorithms in fields such as
robotics, autonomous navigation, and environmental perception, where
understanding complex 3D structures and diverse landscapes is crucial.

\section{Results and Discussion}


\subsection{Performance Vs Distance}
We wanted to see how the performance metrics are affected as the distance of points increases. We can see from the plots that the precision is least affected by increasing
point distance. The recall is slighly reduced with increases distances and this is due to the region growth step in our solution. As the distance between the scan lines
inceases, the far off points beloging to scan lines with gaps greator than the cell size are ignored in the region growth. The reduced recall scores are specially evident
in the Sequence 1 because of gaps in point cloud samples due to occlusions and separatation between the two streets.

\begin{figure}[h!]
  \centering
  \begin{subfigure}[b]{0.5\textwidth}
      \centering
      \includegraphics[width=\textwidth]{figures/graphs/00.jpg}
      \label{fig:subfig1}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.5\textwidth}
      \centering
      \includegraphics[width=\textwidth]{figures/graphs/01.jpg}
      \label{fig:subfig2}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.5\textwidth}
      \centering
      \includegraphics[width=\textwidth]{figures/graphs/04.jpg}
      \label{fig:subfig3}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.5\textwidth}
      \centering
      \includegraphics[width=\textwidth]{figures/graphs/09.jpg}
      \label{fig:subfig4}
  \end{subfigure}
  \caption{A main caption describing all four subfigures}
  \label{fig:main_fig}
\end{figure}


\subsection{Experiements on Datasets}

The ground segmentation on various datasets was performed without any paramter tuning. The ground points [pink] and non-ground points [blue] are shown.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.5\textwidth, height=4cm]{figures/grouped_figures/botanic_garden_all.png}
  \caption{Ground segmentation results from the BotanicGarden dataset}
  \label{fig:botanic_garden}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.5\textwidth, height=4cm]{figures/grouped_figures/doals_all.png}
  \caption{Ground segmentation results from the DOALS dataset}
  \label{fig:doals}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.5\textwidth, height=4cm]{figures/grouped_figures/kitti_all.png}
  \caption{Ground segmentation results from the SemanticKITTI dataset}
  \label{fig:single_fig}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.5\textwidth, height=4cm]{figures/grouped_figures/rellies3d_all.png}
  \caption{Ground segmentation results from the Rellis-3D dataset}
  \label{fig:rellies_3d}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.5\textwidth, height=4cm]{figures/grouped_figures/urban_nav_all.png}
  \caption{Ground segmentation results from the UrbanNav Tokyo dataset}
  \label{fig:urban_nav}
\end{figure}

\subsection{Comparison with Other Solutions}

In the section, we compare the results of GroundGrid3D with other ground segmentation solutions on the SemanticKITTI dataset. This is done due to availability of labelled
data and also because the other solutions show case best performance on SemanticKITTI dataset. Each compare the frames where other solutions fail to give satisfactory
segmentation with out proposed solution. The True Positive [yellow], False Positive [Blue], and False Negative [Purple] points are shown. 

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.5\textwidth]{figures/metrics/group_frame_20.png}
  \caption{Frame 20 of the 0th sequence in the SemanticKITTI dataset}
  \label{fig:group_frame_20}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.5\textwidth]{figures/metrics/group_frame_100.png}
  \caption{Frame 100 of the 0th sequence in the SemanticKITTI dataset. Notice the significantly greater number of false positives [blue] points in the
  Patchwork and Patchwork++ solutions}
  \label{fig:group_frame_100}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.5\textwidth]{figures/metrics/group_frame_140.png}
  \caption{Frame 140 of the 0th sequence in the SemanticKITTI dataset}
  \label{fig:group_frame_140}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.5\textwidth]{figures/metrics/group_frame_150.png}
  \caption{Frame 150 of the 0th sequence in the SemanticKITTI dataset}
  \label{fig:group_frame_150}
\end{figure}

\begin{figure*}[h!]
  \centering
  \includegraphics[width=16cm, height=10cm]{figures/datasets/UrbanNav/combined.png}
  \caption{UrbanNav Tokyo Dataset}
  \label{fig:urbannav_combined}
\end{figure*}


\begin{table}[tbp]
  \centering
  \caption{Comparison of ground segmentation on SemanticKITTI sequence 00 with vegetation}
  \label{tab:comparison_00_non_downsampled}
  \begin{tabular}{@{}ll@{\ \ }l@{\ \ }l@{\ \ }r@{}}
  \toprule
   Algorithm   & Precision$\pm$SD (\%) & Recall$\pm$SD (\%) & F1 (\%) & Time (ms)  \\
\midrule
  GPF          & $94.95\pm 4.99$ & $79.02\pm27.20$ & 86.26 & 13 \\ 
  RGPF         & $59.20\pm11.56$ & $96.57\pm 1.83$ & 73.40 & 18 \\ 
  RANSAC       & $87.52\pm14.63$ & $93.35\pm11.38$ & 90.84 & 64  \\ 
  LineFit      & $98.15\pm 0.97$ & $83.05\pm 9.94$ & 89.97 & 7 \\ 
  Patchwork    & $92.34\pm 3.31$ & $94.61\pm 3.44$ & 93.46 & 17 \\ 
  Patchwork++  & $94.62\pm 2.76$ & $98.73\pm 1.10$ & 96.62 & 11  \\ 
  \bottomrule
  GroundGrid3D & $95.34\pm 3.33$ & $87.02\pm 11.68$ & 91 & 89\\ 
  \bottomrule
\end{tabular}%
\end{table}

\begin{table}[tbp]
  \centering
  \caption{Comparison of ground segmentation on SemanticKITTI sequence 00 with vegetation [Downsampled to 10 cm]}
  \begin{tabular}{@{}ll@{\ \ }l@{\ \ }l@{\ \ }r@{}}
  \toprule
   Algorithm & Precision$\pm$SD (\%) & Recall$\pm$SD (\%) & F1 (\%) & Time (ms)  \\
\midrule
  GPF          & $93.48\pm3.50$ & $76.93\pm21.67$& 84.46 & 5 \\ 
  RGPF         & $52.61\pm9.35$ & $93.59\pm2.92$ & 67.35 & 8 \\ 
  RANSAC       & $83.82\pm12.88$& $92.17\pm7.90$ & 87.80 & 28  \\ 
  LineFit      & $97.34\pm1.25$ & $78.48\pm9.39$ & 86.91 & 3 \\ 
  Patchwork    & $88.62\pm3.96$ & $91.76\pm4.57$ & 90.17 & 7 \\ 
  Patchwork++  & $91.66\pm2.87$ & $97.31\pm2.98$ & 94.40 & 6  \\ 
  \bottomrule
  GroundGrid3D & $93.88\pm3.92$ & $81.83\pm11.57$ & 87.44 & 42\\ 
  \bottomrule
\end{tabular}%
  \label{tab:comparison_00_downsampled}
\end{table}

\begin{table}[h!]
  \centering
  \caption{Runtime of GroundGrid3D on varied datasets}
  \resizebox{\columnwidth}{!}{%
  \begin{tabular}{|p{2cm}|p{2cm}|p{2cm}|p{2cm}|}
  \hline
  \textbf{Dataset} & \textbf{Pointclouds} & \textbf{Average Points} & \textbf{Average Time (ms)} \\
  \hline
  \textbf{SemanticKITTI \cite{todo}} & 500 & 121253 & 86 \\ 
  \hline
  \textbf{UrbanNav \cite{todo}} & 500 & 56639 & 42 \\ 
  \hline
  \textbf{DOALS \cite{todo}} & 500 & 130895 & 60 \\ 
  \hline
  \textbf{BotanicGarden \cite{todo}} & 500 & 25016 & 21 \\
  \hline
  \textbf{RELLIS-3D \cite{todo}} & 500 & 131071 & 82 \\
  \hline
  \end{tabular}%
  }
  \label{tab:phase_behavior}
  \end{table}






\section{Acknowledgment}
\todo{Proudly presented by \ldots{} But funding should be mentioned in footnote on first page}
\iffalse

The preferred spelling of the word ``acknowledgment'' in America is without 
an ``e'' after the ``g''. Avoid the stilted expression ``one of us (R. B. 
G.) thanks $\ldots$''. Instead, try ``R. B. G. thanks$\ldots$''. Put sponsor 
acknowledgments in the unnumbered footnote on the first page.

\fi

\bibliography{paper}


\end{document}
