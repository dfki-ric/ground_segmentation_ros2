\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{subcaption} % For subfigures
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\usepackage[textsize=tiny]{todonotes}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{gensymb}
\usepackage{enumitem}
\usepackage{float}

\bibliographystyle{IEEEtran}

\graphicspath{{figures}}


\newenvironment{smallcases}{\left\{\begin{smallmatrix}}{\end{smallmatrix}\right.}


\begin{document}

\title{GroundSeg3D: A Robust 3D Grid-Based Hybrid Approach for Ground Segmentation in Point Clouds Across Varied Terrains
\thanks{Funded by the German Federal Ministry of Education and Research, grant number: 13N16537.
All code will be published after acceptance.}}

\author{\IEEEauthorblockN{Muhammad Haider Khan Lodhi and Christoph Hertzberg}
\IEEEauthorblockA{\textit{Robotics Innovation Center}, 
German Research Center for Artificial Intelligence (DFKI),
Bremen, Germany\\
Email: Haider\_Khan.Lodhi@dfki.de, Christoph.Hertzberg@dfki.de}%
}%

\maketitle

\begin{abstract}
Ground segmentation in point cloud data is the process of separating the points
into ground and non-ground points. It is a key component in the perception pipeline of
an autonomous system. The state-of-the-art solutions for ground segmentation are often designed for use-cases from
urban environments. The performance of such solutions is tightly linked to certain assumptions about the environment and the
sensor configuration. Additionally, they require hand tuning of parameters. There is a need to have a robust,
multi-terrain capable ground segmentation solution which does not require tuning of parameters. In this work, we provide such a solution for
indoor and outdoor mobile robotics and autonomous driving applications. Our solution
combines the strengths of different approaches as a single solution and produces
good ground segmentation of point cloud data on varied terrains.
\end{abstract}

\begin{IEEEkeywords}
Ground Segmentation, Point Clouds, Mobile Robotics, Autonomous Driving, Rough Terrain, LiDAR, 3D~Grid-Based Approach
\end{IEEEkeywords}


\section{Introduction}
The perception system of an autonomous system utilizes multiple sensors to
capture salient features of the environment. These features are used as inputs by various sub
systems e.g. navigation, control, learning etc. A core sensor of the perception system
is the LiDAR and given the advancements in sensor design and reduced costs, it
has become a standard sensor in autonomous systems \cite{Gomes2023}. The LiDAR uses bands of
laser beams to scan the surrounding environment and the time-of-flight
information is used to capture distance information at high resolution. The
result of the single scan is a point cloud sample which in today’s sensors may
contain more than a million points. Evidently, the point cloud captures the
points from the environment in the scanning region of interest. However, not all 
points in the region of interest are relevant for the downstream algorithms. 
%For example, the obstacle detection 
%algorithm is only interested in the obstacle points and the road detection algorithm only requires the ground points. 
For example, object and obstacle detection algorithms detect ground points as false
    positives. The detection can be improved when ground points are detected and
    removed. Additionally, the computational burden is reduced by processing only
    non-ground points \cite{Firkat2023}.
%
Ground points can be used for traversability analysis, navigation, and
    static map generation.
Detecting them is a necessary precursor step for extraction
    of useful information for downstream tasks in mobile
    robot navigation \cite{Jimenez2021,Arora2023}.
\begin{figure}[b]
  \centering
  \includegraphics[width=\linewidth]{intro_combined.png}
  \caption{GroundSeg3D: The point cloud is segmented into ground points (magenta) and non-ground points (blue). }
  \label{fig:results}
\end{figure}

\iffalse
Feeding the same point cloud sample to both algorithms would require redudant processing of irrelevant points in both 
algorithms. To mitigate the redudancy, it is a usual practice to segment the points into ground and non-ground points. 
The segmentation task is performed by a ground segmentation algorithm and it has many applications:
\begin{itemize}
  \item Ground Detection
  \item Obstacle Detection
  \item Object Detection
  \item Terrain Analysis
  \item Traversability Analysis
  \item Drivable Area Detection
  \item SLAM
\end{itemize}

Ground point extraction from a point cloud sample is a crucial step in the perception pipeline of autonomous systems. 
Numerous solutions have been proposed over the past few years. 
\fi

In this work, our primary focus is to extract ground 
points with minimal parameter dependency, eliminating the need for parameter tuning, and ensure consistent performance across 
varied terrains and sensor configurations. Our algorithm was tested on real-world datasets, featuring different 
types of point cloud sensors in various configurations and environments. 

\iffalse
\subsection{Benefits of Ground Segmentation}
\begin{itemize}
  \item Detection of ground points is a necessary precursor step for extraction
    of useful information from point cloud data for downstream tasks in mobile
    robot navigation \cite{Jimenez2021,Arora2023}.
  \item Object and obstacle detection algorithms detect ground points as false
    positives. The detection can be improved when ground points are detected and
    removed. Additionally, the computational burden is reduced by processing only
    non-ground points \cite{Firkat2023}.
  \item Ground points can be used for traversability analysis, navigation, and
    static map generation.
\end{itemize}
\fi

\section{Related Work}
Existing ground segmentation methods typically rely on either traditional techniques or learning-based approaches. 
This classification is supported by a recent survey on ground segmentation methods \cite{Gomes2023}, which 
categorizes the current state-of-the-art solutions accordingly.

\subsection{Traditional Solutions}
The traditional solutions for ground segmentation use geometric properties for segmentation. Such solutions are usually based 
on occupancy grids or elevation maps \cite{5650541,9558794}. Some
approaches extend the grid cell and analyze the points in neighboring cells for
a better estimation of surface properties \cite{9969541}. The modeling of the points in the
grid cells using  plane fitting and line extraction is a common approach in many
solutions \cite{9466396,9981561}. Region growing and clustering is used to combine individual grid
cells into a larger region of connected cells \cite{5164280}. Higher order inferences based
on Conditional Random Field (CRF) and Markov Random Field (MRF) have also been used
\cite{9410344,7995861}.

\subsection{Learning Solutions}
The advancements in deep learning meant that some researchers used the existing
CNN networks originally used for object detection for ground segmentation using
range images generated from the point cloud data. The development of PointNet \cite{qi2017pointnet} and its successor 
PointNet++ \cite{qi2017pointnet++} gave a unified
solution for various applications e.g. object and 
scene segmentation. Various other solutions like GndNet \cite{paigwar2020gndnet}, PointPillars \cite{lang2019pointpillars}, JCP \cite{rs13163239}, SectorGSnet \cite{9691325}
show good results.

\section{Proposed Solution}
\iffalse
Our goal was to develop a ground segmentation solution based
on the traditional approaches and after analyzing the state-of-the-art, we decided
to combine different approaches as a single solution by utilizing the best of
all approach.

\subsection{Key Challenges in the Existing Traditional Solutions}
We noticed some key shortfalls in the existing solutions and aimed to come up with a solution which can solve the open challenges.
Below we list some of the key challenges. The challenges were found out after careful experimentation of various datasets and sensor 
configurations. We documented the challenges in the experimentaion section at the end of the paper.

The key challenges faced when working with the existing state-of-the-art solutions are that they;
\begin{itemize}
  \item require fine tuning of many algorithm parameters for best performance.
  \item have high degree of performance dependency on the type of sensor used for the point cloud.
  \item are developed for the autonomous driving scenario with only a few solutions showcasing adequate performance
        on rough uneven terrains.
  \item show good performance on the SemanticKITTI datset but do not adapt to other datasets from the autonomous driving domain.
\end{itemize}

Below we discuss some some key challenges which researches try to solve in the
ground segmentation approaches and suggest our selected solution for it.

\subsection{Computational Requirements}
A high-resolution LiDAR sensor can generate millions of data points. For
instance, the Velodyne VLS-128 can produce up to 9.6 M points per second in the
dual-return mode, with frame rates varying from 5 Hz to 10 Hz. In a typical
operation, this sensor can be configured to produce, on average, a point cloud
of 2,403,840 points per second (240,384 points at 10 Hz), which makes it harder
to analyze the entire point cloud in real time during the navigation tasks.

\paragraph*{Solution} We use a grid-based technique in 3D space to help mitigate these problems, 
where each grid cell contain its assigned points and can be processed individually on demand.

\subsection{Terrain Roughness}
The assumption of flat ground surfaces may be plausible in a urban environment but it 
does not always hold. Autonomous systems usually have to navigate 
on uneven terrain with varying degrees of roughness. Therefore, the ground segementation algorithms
are not always easily applicable for msuch systems.

\paragraph*{Solution} We use a combination of local and global terrain properties e.g. surface gradiant,
local spacial distribution of points, and global connectivity. We apply local eigen analysis to extract local 
terrain surface properties and utilize a connectivity-based region growing algorithm based on initial seed selection. 

\subsection{Size of Grid Cell}
We faced the challenge regarding the selection of a suitable grid cell size in 
relation to the roughness of the terrain. In principle, small grid cells are better 
suited for model fitting but the grid cell's size has a direct impact on the computational performance
of the algorithm. The selected size also depends on the gap of the scan lines.
The scan lines in close proximity to the robot are dense and the distance
between the lines increases based on the distance from the robot. Irrespective
of the grid cell size, a cell close to the robot is more likely to have points
from multiple scan lines assigned to it. This means that such cells are better
suited to fit a plane model. As a consequence, it is possible to have an
accurate slope estimate of the local terrain surface. On the contrary, cells which
are assigned points far away from the robot are more likely to have only single
scan lines assigned to them. A plane fitting algorithm can fit a plane to the
line of points but the slope estimate is highly uncertain and is usually
unreliable. Additionally to this, a small cell size is more sensitive to noisy 
sensor data and has high computational costs.

Furthermore, the size of the grid cells is also closely related to the type of
application and environment. In indoor environments with flat surfaces, it is
generally unproblamatic to have large grid cell size as compared to outdoor
uneven terrain. To sum up, there is so one size fits all solution when it comes
to selection of a suitable grid cell size for a ground segmentation algorithm.

\paragraph*{Solution} We make use a phase-wise approach and vary the size of grid
cell. In the first phase, we use a large height for the grid cells to capture as 
many tall structures as possible. The second phase is applied on the resultant 
ground points of the first phase and we use a small height for the grid cells to 
remove any false positive ground points.

\subsection{Point Sparsity}
The point assignment to grid cells is non-uniform and
depends on various factors. It is not feasible to fit a plane to all
grid cells because some cells may have a single line of points or a random
distribution of points. This is especially the case when using a fixed grid size
and the distance of the cell from the point cloud origin increases.

\paragraph*{Solution} We perform local eigen analysis on each grid cell. The eigen values
encode a good representation of the spacial distribution of the points assigned to the cell. 
Afterwards, we classify each grid cell into one of the three classes: Line, Planar, Non-Planar. Each 
cell type is handed with a different technique most appropritate for it. 

\subsection{Polar \& Square Coordinates}
Literature review has shown that researchers prefer polar grid cells as compared
to square cells. Various types of polar grid cells have been researched.
\paragraph*{Solution} However, in our experiments we found a square grid cell
with a fixed grid size performs better than polar cells. The reason in our view
is that as the distance from the sensor origin increases, the cells get bigger
and as a consequence, results in the deterioration of the performance of the
plane fitting based segmentation.

\subsection{False positive flat surfaces e.g. table top, car roof etc.}
The gradient of the fitting plane to a grid cell can not be used as the sole
criteria for the segmentation. The reason is the existence of flat surfaced
obstacles in the robot's environment.
\paragraph*{Solution} We apply region growing based on high confidence intial
ground seed cells. The connectivity-based expansion ensures that only
neighboring ground cells are expanded.

\subsection{False positive points at the junction of ground and non-ground points}
The results of the segmentation depend on the handling of junctions between
ground and non-ground points. This is where the majority of cases of under and over
segmentation occur. 

\paragraph*{Solution} A point-wise inlier check utilizing KDTree’s is used at the
junction of ground and non-ground cells to detect and correct over and under segmentation.

\subsection{Influence of grid cell height on the performance of segmentation}
The height of the grid cell plays a key role in identification of non-ground points in
the point cloud. The key challenge here is to detect tall non-ground objects in the grid
cells but without false detection the underlying ground points as non-ground points.
This challenge is evident in the tree canopy situation where the ground points
in the grid cell are falsely identified as non-ground points because of the large
height of the grid cell.
\paragraph*{Solution} The point space is sub-divided into 3D grid cells for a clear distinction
between ground and non-ground points. Additionally, we use the same strategy as mentioned in the 
point G to detect and correct segmentation mistakes.

\subsection{Fitted Plane Outlier Correction}
The real world ground surface has a curvature and plane fitting usually results in false 
positive outliers upon fitting a ideal flat surface to the the points. 
\paragraph*{Solution} It is common practice to define a distance threshold to fit a planar model to a set of 
points. We also use a fixed distance threshold for the initial plane fit for for gradient estimation. However, the 
extraction of ground and non-ground points uses an additional correction step using the actual resultant normal of 
the points assinged to the cell. We use the strategy as mentioned in the point G to detect and correct segmentation mistakes.

\subsection{Performance}
The requirement for real-time operation of the ground segmentation is very
evident for the application in the autonomous driving community. Therefore,
solutions need to able to do a highly accurate segmentation within in stipulated
time period. The exact time requirements may vary based on the industry.

\paragraph*{Solution} \todo{Time and space complexity analysis needs to be done}

\section{Core Components}
\fi

GroundSeg3D takes a single point cloud and outputs the 
ground and non-ground points. 
%In this section, we explain the key processing steps performed on each input for robust ground segmentation.
%
\iffalse
\subsection{Data Preparation and Filtering}
The input point cloud is cropped to a user-defined region of interest, which varies depending on the application. 
For autonomous driving, a larger region of interest is preferred, while mobile robotics typically require a smaller
to medium-sized region.
\fi
%\subsection{Two-Phase Segmentation of Points Based on Local Surface Properties and Neighborhood Analysis}
%
The segmentation process is divided into two distinct phases (Fig.\@~\ref{fig:phases}).
%each phase uses grid cells of varying heights to improve the accuracy of ground and non-ground detection, 
%while also managing potential false segmentation as shown in Fig.\@~\ref{fig:dual_phase_segmentation}.
\begin{figure}[tb]
  \centering
  \includegraphics[width=\linewidth]{figures/overview/stages.png}
  \caption{A two phase approach is developed for robust segmentation of ground and non-ground points. Phase-I uses grid cells with large height
  to capture tall structures as non-ground points. The coarse ground points from Phase-I are subsequently used as
  input for Phase-II and processed using grid cells with small height.}
  \label{fig:phases}
\end{figure}
%
In the first phase of segmentation, a large height for the grid cells is 
used to capture as many non-ground structures as possible within each grid cell. However, 
the increased cell height can lead to false segmentation of ground points, 
especially where the ground is obscured by overhead objects. 
These false detections are addressed in subsequent stages of the algorithm.

The second phase uses a smaller grid cell height to 
refine the segmentation results from the first phase. 
In this phase, false positive ground points from Phase~I are identified as obstacle points and false positive 
non-ground points are identified as ground points based on neighborhood correction.

\iffalse
\begin{figure}[tb]
  \centering
  \includegraphics[width=\linewidth]{figures/overview/pipeline.png}
  \caption{The core stages in the pipeline of GroundSeg3D. The same steps in the pipeline are used in both phases with slight modifications.}
  \label{fig:pipeline}
\end{figure}
\fi
Table~\ref{tab:phase_behavior} shows the variation in the processing pipelines of in the two phases.
The steps are described in the following sections.

\begin{table}[tb]
  \centering
  \caption{Behavior of steps in the first and second phase of segmentation}
  \label{tab:phase_behavior}
%  \todo[inline]{Only write this inside text}
  \begin{tabular}{@{}p{3.3cm}p{1.8cm}p{2.9cm}@{}}
\toprule
  \textbf{Step} & \textbf{First Phase} & \textbf{Second Phase} \\
\midrule
  \textbf{Grid Representation} & Grid cells have a large height & Grid cells have a small height \\
  \textbf{Local Eigen Analysis} & Unchanged & Unchanged \\
  \textbf{Surface Gradient Analysis} & Unchanged & Unchanged \\
  \textbf{Ground Region Growth} & Unchanged & A detailed neighborhood check is performed for neighboring cells during expansion \\
  \textbf{Final Ground Segmentation\!\!\!} & Unchanged & Unchanged \\
\bottomrule
  \end{tabular}%
\end{table}

\subsection{Grid Representation}
The points are assigned to cells within a 3D grid 
to model and detect empty spaces. 
\begin{align}
cell_{\{x,y,z\}} &= \lfloor point_{\{x,y,z\}} / cellsize_{\{x,y,z\}}\rfloor,
%cell_y &= point_y / cellsize_y \\
%cell_z &= point_z / cellsize_z
\end{align}
where $cellsize$ is a parameter. 
%We explain our choice of parameters in section \ref{sec:parameters}.

\subsection{Local Eigen Analysis}
We use local eigen 
analysis to estimate the local spatial distribution of points within each grid cell.
For each grid cell, we calculate the eigenvalues of the covariance matrix of the points.
Let $\lambda_1 > \lambda_2 > \lambda_3$ be the eigenvalues, we then compute the ratio:
%
%We use the ratio of the largest eigenvalue to the sum of all eigenvalues to gauge the point distribution:
%
\begin{align}
  \text{Ratio} &= \tfrac{\lambda_1}{\lambda_1 + \lambda_2 + \lambda_3}
\end{align}  
%
The ratio of the largest eigenvalue to the sum of all eigenvalues provides an indication of whether the points in 
a grid cell are more aligned with a line, a planar surface, or a non-planar structure.

%TODO make sure it's in remaining text
\iffalse
\begin{description}%[style=nextline]
  \item[Line Cell]
  A high ratio in range $[0.9, 1.0]$ indicates that $\lambda_1$ is significantly larger than $\lambda_2$ and $\lambda_3$, which suggests that the points 
  are distributed along a line. In this case, the distribution is highly elongated along one direction with minimal variation
  in the other directions.
%
  \item[Planar Cell]
  A ratio in this range $[0.4, 0.9)$ indicates that $\lambda_1$ is moderately larger than $\lambda_2$ and $\lambda_3$, suggesting that the points 
  are more aligned with a planar surface.
%
  \item[Non-planar Cell]
  A ratio in this range $[0.0, 0.4)$ indicates that $\lambda_1$ is relatively smaller than $\lambda_2$ and $\lambda_3$, suggesting that the points 
  are more aligned with a non-planar surface.
\end{description}
\fi
\iffalse
\begin{table}
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Cell Type} & \textbf{Threshold} \\
\midrule
Line Cell      & $> 0.95$ \\
Plane Cell     & $\ge 0.40$ \\
Non-planar Cell   & $< 0.40$ \\
\bottomrule
\end{tabular}
\caption{Largest eigen to the sum ratio}
\label{tab:eigen-ratio}
\end{table}
\fi

%Each type of cell is processed using a distinct algorithm, and we refer to these cells as Line, Plane, and Non-Planar cells:

\subsubsection{Line Cell}

\begin{figure}
  \centering
\begin{subcaptiongroup}
  \includegraphics[width=\linewidth]{celltypes.png}
  \phantomcaption\label{fig:LineCell}
  \phantomcaption\label{fig:PlaneCell}
  \phantomcaption\label{fig:UnknownCell}
\end{subcaptiongroup}
%   \subcaptionbox{Line Cell\label{fig:LineCell}}[0.3\linewidth]{
%   \includegraphics[width=\linewidth]{line_cell.png}
%   }
%   \hfil
%   \subcaptionbox{Plane Cell\label{fig:PlaneCell}}[0.3\linewidth]{
%   \includegraphics[width=\linewidth]{plane_cell.png}
%   }
%   \hfil
%   \subcaptionbox{Non-Planar Cell\label{fig:UnknownCell}}[0.3\linewidth]{
%   \includegraphics[width=\linewidth]{unknown_cell.png}
%   }
  \caption{Points (black) assigned to a grid cell and the 
  dominating Eigen vectors (green) are shown. 
  The cell is classified as Line~(\subref{fig:LineCell}), Plane~(\subref{fig:PlaneCell}),
  or Non-Planar~(\subref{fig:UnknownCell})
  based of the local eigen analysis.}
\end{figure}

If the largest eigenvalue is significantly larger than the other eigenvalues, the cell is categorized as a Line Cell.
For these cells, we compute the angle between the largest eigenvector 
(depicted in green in Fig.~\ref{fig:LineCell}) and the $z$-axis.
%
\begin{align}
CellType &= 
  \begin{smallcases}
    \text{Obstacle}         & angle \ge 90\degree - Threshold \\
    \text{Tentative Ground} & angle < 90\degree - Threshold
  \end{smallcases}
\end{align}
%
where $Threshold$ refers to the ground slope threshold.

The ground slope threshold is a parameter and is used to determine the classification of cells based on their angle of uprightness. 
% Cells with a small angle between the largest eigenvector and the positive z-axis are considered for classification
% as obstacles. This is because ground cells typically do not exhibit a highly upright spatial distribution.
For tentative ground cells we cannot be certain of their exact nature.  
As a result, such cells are marked
as tentative ground until it is clarified in the region growth step (Section~\ref{sec:region_growth})


However, because this configuration could also represent the top of an
obstacle (such as a car roof or a table), we cannot be certain of its exact nature. 

\subsubsection{Planar Cell}
% \begin{figure}
%   \centering
%   \includegraphics[width=\linewidth]{plane_cell.png}
%   \caption{Points (white) assigned to a grid cell and the corresponding eigen vectors (black) are shown. 
%   Two dominant eigen vectors and shown and the third vector is negligible in comparison. 
%   The cell is classified as Plane based on the local eigen analysis.}
%   \label{fig:PlaneCell}
% \end{figure}
Cells that contain multiple scan lines are suitable candidates for plane fitting when the spatial distribution of the points 
exhibits two dominant eigenvalues, with the third eigenvalue being negligibly small or significantly smaller. The Plane cells at 
this stage can not be considered as definitive ground. It might be the case that the cell
exhibiting a planar distrubtion of points is a car roof top or the top of an obstacle. 
%\todo{Suggestion: One could check lowest EV/EV sum $<$ threshold}

\subsubsection{Non-planar Cell}
% \begin{figure}
%   \centering
%   \includegraphics[width=\linewidth]{unknown_cell.png}
%   \caption{Points (white) assigned to a grid cell and the corresponding eigen vectors (black) are shown. 
%   There is no dominance of a single or two eigen vectors. }
%   \label{fig:UnknownCell}
% \end{figure}

If a cell has three dominant eigenvalues, the cell is classified as non-planar, suggesting that the points do not align well with a planar surface.

\subsection{Planar Model Fitting}
\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{plane_model_fit.png}
  \caption{Plane model fit for local surface gradient estimation.}
  \label{fig:PlaneFit}
\end{figure}

The plane model fitting is applied only to cells categorized as Planar Cells. We compute the gradient of 
the fitted plane and apply a slope threshold to assess whether the cell meets the ground criteria. The resultant ground cells 
are used as candidates for initial seeds in the region-growing algorithm.
%
\begin{align}
CellType &= 
  \begin{smallcases} 
    \text{Tentative Ground}, & angle \le Threshold \\
    \text{Non-Ground}, & angle > Threshold
  \end{smallcases}
\end{align}
where $Threshold$ refers to the same ground slope threshold used in (Section~\ref{sec:}).

\subsection{Seed Selection}
The seed cells are selected from the tentative ground cells based on the following conditions:

\begin{enumerate}
  \item \textbf{High Confidence of Ground Classification:} The cell is high confidence when a large percentage of total points 
  are inliers of the planar model fit.
  
  \item \textbf{Proximity to Sensor:} The centroid of the cell must be within a user-defined distance threshold. Cells that are far 
  from the sensor typically have sparse points with large gaps, making them unsuitable for initial seed selection due to limited 
  connectivity with neighboring cells.
  
  \item \textbf{Quadrant-Based Seed Selection:} Seed cells are selected from each of the four quadrants in the Euclidean space. 
  This approach helps mitigate issues related to large occlusions or gaps in the point cloud, which could inhibit region growth. 
  By selecting seed cells from four distinct groups (one per quadrant), we improve the 
  coverage and connectivity of the region-growing process.
   
  \item \textbf{Selection of a Single Seed Cell:} From each valid group, one seed cell is selected based on the following criteria:
  \begin{itemize}
  \item The cell must have the highest number of ground neighbors compared to other cells in the group.
  \item It should be closest to the mean height of the group.
  \end{itemize}
  
  \item \textbf{Final Seed Cells:} After shortlisting seed cells in all quadrants, we select the lowest cell from the front (1st and 2nd quadrants) 
  and the back (3rd and 4th quadrants). These two seed cells are then used for region growing.
  \end{enumerate}

\subsection{Ground Region Growth}\label{sec:region_growth}
We recursively expand all ground neighbors of the seed cells. This region-wise growth process ensures that cells falsely 
classified as ground cells (i.e., cells that meet the slope criteria but are not truly ground) are not included in the final result.

When expanding the neighbors of a cell, we consider neighboring cells in its vicinity. Empty cells are not stored in the 3D grid, 
which helps in focusing only on relevant neighboring cells. Each ground neighbor within the distance threshold is expanded and 
subsequently marked as expanded. This designation prevents any cell from being expanded more than once, ensuring efficiency 
and accuracy in the expansion process.

Phase II includes an additional step where points from neighboring cells above or below the expanded cell are checked. If most points in a neighboring cell 
are within the ground inlier threshold, that cell is selected for further expansion. Conversely, if the majority of a neighboring cell's points are not 
within the ground inlier threshold, the cell is not expanded. This step prevents expanding cells that meet the slope threshold criteria but are actually obstacles.


\subsection{Final Ground Segmentation}

The final ground segmentation is a critical step in the pipeline for accurately interpreting ground points from the input point cloud data, 
especially at junctions of ground and obstacle cells. This process involves analyzing and classifying the points in the cells based on
their surface and neightborhood properties. The goal is to refine the cells obtained from previous steps in the pipeline and ensure 
that each point in the cells is accurately categorized, considering both its local properties and its relationship with neighboring cells.

\subsubsection{Ground Points}

In our approach to point segmentation, each ground cell obtained from the region-growing phase is processed individually. 
This helps us determine whether any outliers from the plane fitting were incorrectly marked. Specifically, we use the smallest 
eigenvalue of the points in the cell, rather than the normal to the plane fit, to check for false negatives. This pointwise check uses KDTree's and
helps in identifying and correcting false outliers.

The process involves the following steps:

\paragraph*{Outlier Check}

\begin{enumerate}
\item Identify the closest inlier point and compute the vector from this inlier to the outlier.
\item Calculate the component of this vector along the ground normal.
\item Based on the distance along the normal:
\begin{enumerate}
\item If the distance is less than or equal to the plane fit threshold, the outlier is classified as a ground point.
\item If the distance exceeds the plane fit threshold, the outlier is classified as a non-ground point.
\end{enumerate}
\end{enumerate}

\subsubsection{Non-Ground Cells}

Segmentation of non-ground cells presents additional challenges, especially at the boundaries where obstacles meet ground surfaces. 
To address these challenges, we perform the following checks:

\begin{enumerate}
\item Neighbor Check: We first check if a non-ground cell has any ground neighbors. This step is crucial because false segmentation 
often occurs at the junctions between obstacles and ground cells.
\begin{itemize}
\item If the non-ground cell has no ground neighbors, all its points are classified as non-ground.
\item If the non-ground cell does have ground neighbors, we need to perform further analysis to determine the validity of these neighbors.
\end{itemize}
\item Pointwise Check: For non-ground cells with ground neighbors, we perform a pointwise comparison. This involves analyzing the 
obstacle points in the cell and comparing them with neighboring ground points. The goal is to identify and remove any false positive obstacle points.
\end{enumerate}

By implementing these procedures, we refine the segmentation of non-ground cells and improve the overall accuracy of the point 
classification. This approach helps in reducing false positives and ensuring that the point cloud data accurately reflects the 
true nature of the environment.

\section{Experiments}

The goal of experimentation was to test not only the ground segmentation but also the adaptability to different environments and sensor configurations. For comparison,
we selected some of the state-of-the-art ground segmentation works form literature. We used the standard parameters for the algorithms. Some algorithms require 
the sensor height as a parameter. Therefore, sensor height was the only parameter which was updated. Our proposed solution does not require sensor height as a parameter 
and we did not update any parameters in our tests throughout the experimentation. The results of our GroundSeg3D are with the same set of paramters shown in table.

The experimentation was performed exclusively on datasets with real world point cloud data. No simulation data was used during the tests. We selected a diverse set of datasets from urban and natural environments. Additonally, the datasets are based on different sensor configurations.
A ground segmentation solution must be able to perform robustly for various sensor types and configurations without parameter tuning. To showcase that our solution is not
bound to a sensor height, we selected a handheld dataset to test the adaptability of our ground segmentation to a changing height of the sensor. In contrast, most existing 
solutions require a sensor height as a parameter with the assumption that the height of the sensor is unchanged. This assumption does not hold in case of handheld or UAV 
based point cloud datasets. Futhermore, the constant sensor height assumption does not hold for mobile robots with the capability to change their height.

\subsection{Datasets}
We used a varied set of available datasets to test the adaptability of GroundSeg3D to new sensors and environments. The datasets provide data from
urban and natural environments. Each dataset uses a different type of sensors. The platforms used for data collection are Car, Mobile Robot and Handheld.
The selection of the handheld dataset was to test how well the solutions perform in varying sensor heights.

\begin{table}[tb]
  \centering
  \caption{Datasets from natural and urban environments}
  \label{tab:datasets}
  \begin{tabular}{@{}p{2.2cm}p{0.9cm}p{1.15cm}p{0.4cm}p{1.1cm}p{0.9cm}@{}}
  \toprule
  \textbf{Dataset} & \textbf{Environ-ment} & \textbf{Sensor} & \textbf{Year} & \textbf{Platform} & \textbf{Sensor Height}\\
  \midrule
  {SemanticKITTI\,\cite{Geiger2013IJRR}\!\!\!\!\!} & Urban & HDL-64E & 2013 & Car & 1.72\,m \\ 
  {UrbanNav\,\cite{Hsunavi.602}} & Urban & HDL-32E & 2021 & Car & 2.1\,m \\ 
  {DOALS\,\cite{9560730}} & Urban & OS0 128 & 2021 & Handheld & 2.0\,m \\ 
  {RELLIS-3D\,\cite{9561251}} & Natural & OS1 64 & 2022 & Mob.\,Robot & 1.16\,m \\
  {BotanicGarden\,\cite{10415477}\!\!} & Natural  & VLP-16 & 2024 & Mob.\,Robot & 1.16\,m \\
  \bottomrule
  \end{tabular}%
\end{table}
\iffalse
\subsubsection{SemanticKITTI}
The SemanticKITTI dataset \cite{Geiger2013IJRR} is a comprehensive collection of real-world data designed to
facilitate research in autonomous driving and computer vision. Collected by the
Karlsruhe Institute of Technology and the Toyota Technological Institute at
Chicago, it includes data captured from a vehicle equipped with various sensors,
such as high-resolution stereo cameras, 3D laser scanners, and GPS/IMU systems.
This dataset covers a wide range of urban, suburban, and rural environments,
providing benchmarks for tasks such as object detection, tracking, segmentation,
and visual odometry. Its diversity and complexity make it a pivotal resource for
advancing autonomous driving technologies and robust computer vision algorithms.

\subsubsection{UrbanNav}
The UrbanNav dataset, available on GitHub, is a comprehensive dataset designed for urban navigation research, 
particularly in Global Navigation Satellite System (GNSS)-denied environments. Developed by the Intelligent 
Positioning and Navigation Laboratory at The Hong Kong Polytechnic University, it provides data collected 
from various sensors, including GNSS receivers, Inertial Measurement Units (IMUs), Light Detection and Ranging (LiDAR) sensors, 
and cameras, in dense urban settings like Hong Kong. The dataset is intended to facilitate the development and testing of robust 
navigation algorithms that can handle the challenges of urban environments, such as multipath effects and signal blockages. The 
repository includes detailed descriptions of the data collection process, file structure, and usage guidelines, making it a valuable 
resource for researchers working on urban navigation and related fields.

\subsubsection{DOALS}
Dynamic Object Aware LiDAR SLAM based on Automatic Generation of Training Data (DOALS) contains 12000 scans that were recorded
in the main hall of ETH Zurich. The recording was performed at various locations and each location provides 2 sequences of data.
We use the data collection from the touristic pedestrian zone (Nierendorf) for our ground segmentation tests.

\subsubsection{BotanicGarden}
The BotanicGarden dataset is a used to test the ground segmentation capabilities of the algorithm in unstructured natual environments. The
dataset provides data from wide varierty of sensors, including high-res and high-rate stereo gray and RGB cameras, 
spinning and MEMS 3D LiDARs, and low-cost and industrial-grade IMUs, supporting a wide range of applications. 

\subsubsection{RELLIS-3D}
The Rellis-3D dataset, developed by Texas A\&M University, is an advanced dataset
aimed at improving semantic segmentation in outdoor environments. Named after
the RELLIS Campus, it features high-resolution LiDAR and camera data captured in
diverse weather conditions and various terrain types. Unlike many other datasets
focused on urban settings, Rellis-3D emphasizes off-road scenarios, including
complex and cluttered scenes with vegetation, rough terrain, and obstacles. This
dataset is instrumental for developing and testing algorithms in fields such as
robotics, autonomous navigation, and environmental perception, where
understanding complex 3D structures and diverse landscapes is crucial.
\fi
\subsection{Performance Metrics}

We use precision and recall to compare the algorithms.
\begin{align}
\text{Precision} &= \tfrac{\text{NTP}}{\text{NTP} + \text{NFP}}, &
\text{Recall} &= \tfrac{\text{NTP}}{\text{NTP} + \text{NFN}},
%\text{F1} &= 2 \cdot \tfrac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}},
\end{align}
where
NTP, NFP, and NFN are the Number of True Positives (correctly predicted positives), 
False Positives (incorrectly predicted positives)
and False Negatives (positives that were incorrectly predicted as negative).

Not all datasets provided labelled point clouds, therefore the performance metrics were 
collected only on the SemanticKITTI dataset. Visual analysis of ground segmentation results on other datasets was enough to compare the performance of GroundSeg3D and
state-of-the-art solutions.

\iffalse
\subsection{GroundSeg3D Parameters}\label{sec:parameters}
One of our main objectives was to develop an algorithm which does not require parameter tuning across various environments and sensor configurations. We achieved our target and
all the experimental results from various datasets use a single set of parameters. 

\begin{table}[tb]
  \centering
  \caption{Parameters of GroundSeg3D}
  \label{tab:parameters}
  \begin{tabular}{@{}p{3cm}p{1cm}p{4cm}@{}}
\toprule
  \textbf{Parameter} & \textbf{Value} & \textbf{Description} \\
\midrule
  \textbf{max\{X,Y,Z\}, min\{X,Y,Z\}} & $\pm80$ & Region of interest (in meters) of the point cloud in Euclidean space. \\ 
  \textbf{downsample} & False & Toggle to downsample the point cloud to a user-defined resolution in meters. \\ 
  \textbf{downsample\_resolution} & 0.1 & Downsample point cloud to 10\,cm. \\ 
  \textbf{(cellSizeX, cellSizeY, cellSizeZ)} & 2, 2, 10 & Size (in meters) of the grid cell in three-dimensional space. \\ 
  \textbf{startCellDistanceThreshold} & 10 & Maximum distance for selection of seed cells used as start cells for region growing. \\
  \textbf{groundInlierThreshold} & 0.1 & Inlier threshold (in meters) for the plane model fitting to points in a grid cell. \\ 
\bottomrule
  \end{tabular}%
  \label{tab:parameters}
\end{table}
\fi
\section{Results and Discussion}

\subsection{Performance on varied terrains}

The ground segmentation on various datasets was performed without any paramter tuning. 
The ground points [pink] and non-ground points [blue] are shown in figures \ref{fig:single_fig} to~\ref{fig:botanic_garden}. 

\begin{figure}[tb]
  \centering
  \includegraphics[width=\linewidth,trim={0 0 0 1.5cm},clip]{figures/grouped_figures/kitti_all.png}
  \caption{GroundSeg3D results on the SemanticKITTI dataset}
  \label{fig:single_fig}
\end{figure}

\begin{figure}[tb]
  \centering
  \includegraphics[width=\linewidth,trim={0 0 0 1.5cm},clip]{figures/grouped_figures/urban_nav_all.png}
  \caption{GroundSeg3D results on the UrbanNav Tokyo dataset}
  \label{fig:urban_nav}
\end{figure}

\begin{figure}[tb]
  \centering
  \includegraphics[width=\linewidth,trim={0 0 0 1.5cm},clip]{figures/grouped_figures/doals_all.png}
  \caption{GroundSeg3D results on the DOALS dataset}
  \label{fig:doals}
\end{figure}


\begin{figure}[tb]
  \centering
  \includegraphics[width=\linewidth,trim={0 0 0 1.5cm},clip]{figures/grouped_figures/rellies3d_all.png}
  \caption{GroundSeg3D results on the Rellis-3D dataset}
  \label{fig:rellies_3d}
\end{figure}

\begin{figure}[tb]
  \centering
  \includegraphics[width=\linewidth,trim={0 0 0 1.5cm},clip]{figures/grouped_figures/botanic_garden_all.png}
  \caption{GroundSeg3D results on the BotanicGarden dataset}
  \label{fig:botanic_garden}
\end{figure}

\iffalse
\begin{figure}[tb]
  \centering
  \includegraphics[width=\linewidth,trim={0 0 0 1.5cm},clip]{figures/grouped_figures/ric.png}
  \caption{Ground segmentation results from data collected at DFKI RIC - MFH}
  \label{fig:ric}
\end{figure}
\fi
We further analyze the runtime of GroundSeg3D on various datasets (Table~\ref{tab:runtime}). This should give the end user a good idea of the expected runtime.

\begin{table}[tb]
  \centering
  \caption{Runtime of GroundSeg3D on varied datasets}
  \resizebox{\columnwidth}{!}{%
  \begin{tabular}{|p{2cm}|p{2cm}|p{2cm}|p{2cm}|}
  \hline
  \textbf{Dataset} & \textbf{Pointclouds} & \textbf{Average Points} & \textbf{Average Time (ms)} \\
  \hline
  \textbf{SemanticKITTI} & 500 & 121253 & 86 \\ 
  \hline
  \textbf{UrbanNav} & 500 & 56639 & 42 \\ 
  \hline
  \textbf{DOALS} & 500 & 130895 & 60 \\ 
  \hline
  \textbf{BotanicGarden} & 500 & 25016 & 21 \\
  \hline
  \textbf{RELLIS-3D} & 500 & 131071 & 82 \\
  \hline
  \end{tabular}%
  }
  \label{tab:runtime}
  \end{table}

%\subsection{Dual-Phase Ground Segmentation}

\begin{figure}[tb]
  \centering
  \includegraphics[width=\linewidth,trim={0 0 0 1.5cm},clip]{figures/corrections/dual_phase_segmentation.png}
  \caption{Correction of over-segmentation of ground points based on dual-phase segmentation}
  \label{fig:dual_phase_segmentation}
\end{figure}

%\subsection{Performance on Rough Terrain}

\iffalse
\begin{figure}[tb]
  \centering
  \includegraphics[width=\linewidth]{figures/dfki_track/actual_backyard.png}
  \caption{Rough outdoor terrain at the Robotics Test Track on DFKI RIC}
  \label{fig:arter_all}
\end{figure}
\fi
\begin{figure}[tb]
  \centering
  \includegraphics[width=\linewidth,trim={0 0 0 0cm},clip]{figures/sloped terrain/arter_all.png}
  \caption{GroundSeg3D results on rough outdoor terrain at the Robotics Test Track on DFKI RIC}
  \label{fig:arter_all}
\end{figure}

%\subsection{Performance on Sloped Terrain}

\begin{figure}[tb]
  \centering
  \includegraphics[width=\linewidth]{figures/sloped terrain/kitti_all.png}
  \caption{GroundSeg3D results on sloped outdoor terrain in SemanticKITTI}
  \label{fig:kitti_sloped_terrain}
\end{figure}

\iffalse
%\subsection{Performance on Rocky and Flat Terrains}

\begin{figure}[tb]
  \centering
  \includegraphics[width=\linewidth]{figures/dfki_track/mfh.png}
  \caption{GroundSeg3D results on various types of ground surfaces}
  \label{fig:mfh_all}
\end{figure}
\fi

\iffalse
%\subsection{GroundSeg3D Performance vs. Distance}

In this section, we discuss the experimentation results. We take the SemanticKITTI sequence 0 to analyze the performance metrics with the state-of-the-art solutions. We wanted 
to see how the performance metrics are affected as the distance of points increases. We can see from the plots that the precision is least affected by increasing
point distance. The recall is slighly reduced with increases distances and this is due to the region growth step in our solution. As the distance between the scan lines
inceases, the far off points beloging to scan lines with gaps greator than the cell size are ignored in the region growth. The reduced recall scores are specially evident
in the Sequence 1 because of gaps in point cloud samples due to occlusions and separatation between the two streets. We believe that although the reduced recall is compensated
by the capability of GroundSeg3D to exhibit robust performance on varied terrains without parameter tuning and dependence to a particular sensor configuration.

\begin{figure}[tb]
  \centering
  \begin{subfigure}[b]{0.5\textwidth}
      \centering
      \includegraphics[width=\textwidth]{figures/graphs/00.jpg}
      \label{fig:subfig1}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.5\textwidth}
      \centering
      \includegraphics[width=\textwidth]{figures/graphs/01.jpg}
      \label{fig:subfig2}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.5\textwidth}
      \centering
      \includegraphics[width=\textwidth]{figures/graphs/04.jpg}
      \label{fig:subfig3}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.5\textwidth}
      \centering
      \includegraphics[width=\textwidth]{figures/graphs/09.jpg}
      \label{fig:subfig4}
  \end{subfigure}
  \caption{Performance metrics of GroundSeg3D vs. Distance}
  \label{fig:main_fig}
\end{figure}
\fi
%\subsection{Neighborhood Correction}

\begin{figure}[tb]
  \centering
  \includegraphics[width=\linewidth,trim={0 0 0 1.5cm},clip]{figures/corrections/neighborhood_correction.png}
  \caption{Correction of under-segmentation of ground points based on neighboring cells}
  \label{fig:neighborhood_correction}
\end{figure}

%\subsection {Correction of False Outliers}

As shown in Fig.\@~\ref{fig:false_outliers1}, plane fitting to planar cells leads to false 
outliers because the fitted plane does not accurately model the spatial distribution of the 
points in the cell. Therefore we use the Eigen normal to correct these points (Fig.\@~\ref{fig:false_outliers3}).

\begin{figure}[tb]
  \centering
  \includegraphics[width=\linewidth]{figures/corrections/false_outliers/overview_1.png}
  \caption{False outliers (non-ground points) from RANSAC plane fitting on uneven terrain}
  \label{fig:false_outliers1}
\end{figure}

\iffalse
\begin{figure}[tb]
  \centering
  \includegraphics[width=\linewidth]{figures/corrections/false_outliers/overview_2.png}
  \caption{Overview2}
  \label{fig:false_outliers2}
\end{figure}
\fi

\begin{figure}[tb]
  \centering
  \includegraphics[width=\linewidth]{figures/corrections/false_outliers/overview_3.png}
  \caption{Correction of false outliers using Eigen normal (Eigen vector of the smallest Eigen value) instead of plane normal}
  \label{fig:false_outliers3}
\end{figure}

\subsection{Comparison with the state-of-the-art}

In the section, we compare the results of GroundSeg3D with other ground segmentation solutions on the SemanticKITTI dataset. The True Positive [yellow], False Positive [Blue], 
and False Negative [Purple] points are shown. For the sake of conciseness, we selected only a single frame for each solution where the solution under performs. We compare the
segmentation results of our GroundSeg3D at those frames.

\begin{figure}[tb]
  \centering
  \includegraphics[width=\linewidth,trim={0 0 0 1.5cm},clip]{figures/metrics/group_frame_20.png}
  \caption{Frame 20 of the 0th sequence in the SemanticKITTI}
  \label{fig:group_frame_20}
\end{figure}

\begin{figure}[tb]
  \centering
  \includegraphics[width=\linewidth,trim={0 0 0 1.5cm},clip]{figures/metrics/group_frame_100.png}
  \caption{Frame 100 of the 0th sequence in the SemanticKITTI. Notice the significantly greater number of false positives [blue] points in the
  Patchwork and Patchwork++ solutions}
  \label{fig:group_frame_100}
\end{figure}

\begin{figure}[tb]
  \centering
  \includegraphics[width=\linewidth,trim={0 0 0 1.5cm},clip]{figures/metrics/group_frame_140.png}
  \caption{Frame 140 of the 0th sequence in the SemanticKITTI}
  \label{fig:group_frame_140}
\end{figure}

\begin{figure}[tb]
  \centering
  \includegraphics[width=\linewidth,trim={0 0 0 1.5cm},clip]{figures/metrics/group_frame_150.png}
  \caption{Frame 150 of the 0th sequence in the SemanticKITTI}
  \label{fig:group_frame_150}
\end{figure}

\iffalse
\begin{figure*}
  \centering
  \includegraphics[width=16cm, height=8cm]{figures/datasets/UrbanNav/combined.png}
  \caption{Comparison with state-of-the-art solutions on UrbanNav Tokyo Dataset}
  \label{fig:urbannav_combined}
\end{figure*}

\begin{figure*}
  \centering
  \includegraphics[width=16cm, height=8cm]{figures/datasets/BotanicGarden/combined.png}
  \caption{Comparison with state-of-the-art solutions on BotanicGarden Dataset}
  \label{fig:botanicgarden_combined}
\end{figure*}
\fi

We get the third highest precison only second to the LineFit and GPF algorithms and fourth highest F1 score. The recall has a high standard deviation because of the inherent nature
of the region growth algorithm. The state-of-the-art solutions have a clear advantage in terms of runtime over GroundSeg3D. The precision are recall of GroundSeg3D are 
on par with the other solutions. Furthermore, since GroundSeg3D is not only developed for SemanticKITTI and does not require any parameter tuning, we feel that the 
relatively lower recall score is good trade off with the robust applicablity in urban and natural environments.

\iftrue
%combined table
\begin{table*}[tb]
  \centering
  \caption{Comparison of ground segmentation on SemanticKITTI sequences 00, 01, 04, and 09 with vegetation}
  \label{tab:comparison_non_downsampled}
  \begin{tabular}{@{}l@{} l@{\ \ }l@{\ \ }l@{\ \ }l   l@{\ \ }l@{\ \ }l@{\ \ }l   rrrr}
  \toprule
   Algorithm   & \multicolumn{4}{c}{Precision$\pm$SD (\%)} & \multicolumn{4}{c}{Recall$\pm$SD (\%)} & \multicolumn{4}{c}{Time (ms)}  \\
   \cmidrule(lr){2-5} \cmidrule(lr){6-9} \cmidrule(lr){10-13}
   \multicolumn{1}{r}{Sequence:}& 00 & 01 & 04 & 09 & 00 & 01 & 04 & 09 & 00  & 01 & 04 & 09\\
\midrule
  GPF          & $94.9\pm 5.0$ & $92.1\pm 8.5$ & $95.4\pm 5.0$ & $96.0\pm 6.2$ & $79.0\pm27.2$  & $23.5\pm21.2$  & $72.6\pm28.7$ & $63.5\pm29.5$ & 13 & 11& 12 & 12 \\ 
  R-GPF        & $59.2\pm 11.6$& $89.7\pm 6.0$ & $84.6\pm 9.0$ & $71.4\pm 12.1$& $96.6\pm 1.8$  & $89.7\pm 4.6$  & $93.4\pm 1.7$ & $95.4\pm 3.4$ & 18 & 18& 18 & 19 \\ 
  GPR          & $81.0\pm 7.5$ & $92.5\pm 4.5$ & $95.0\pm 2.0$ & $00.0\pm 0.0$ & $95.9\pm2.5$   & $89.6\pm4.9$   & $94.4\pm1.5$  & $00.0\pm0.0$  & 11 & 12& 13 & ?\\ 
  CascadedSeg  & $91.7\pm 9.0$ & $97.1\pm 2.9$ & $98.2\pm 2.4$ & $94.8\pm 5.8$ & $71.0\pm10.4$  & $73.4\pm10.3$  & $72.5\pm3.7$  & $70.1\pm8.0$  & 65 & 64& 70 & 84 \\ 
  RANSAC       & $87.5\pm 14.6$& $96.5\pm 2.5$ & $96.0\pm 2.0$ & $93.3\pm 7.8$ & $93.3\pm11.4$  & $96.0\pm3.8$   & $97.0\pm1.3$  & $89.3\pm6.2$  & 64 & 64& 42 & 64  \\ 
  LineFit      & $98.1\pm 1.0$ & $99.0\pm 1.5$ & $99.5\pm 0.5$ & $98.0\pm 1.3$ & $83.1\pm 9.9$  & $74.2\pm 9.5$  & $80.1\pm 4.6$ & $80.3\pm 9.0$ & 7  & 6 & 6 & 6  \\ 
  Patchwork    & $92.3\pm 3.3$ & $96.0\pm 3.5$ & $97.4\pm 1.4$ & $92.7\pm 3.3$ & $94.6\pm 3.4$  & $89.1\pm 5.2$  & $91.2\pm 2.8$ & $92.2\pm 4.6$ & 17 & 16& 17 & 17 \\ 
  Patchwork++  & $94.6\pm 2.8$ & $98.4\pm 0.7$ & $98.2\pm 0.8$ & $95.4\pm 3.4$ & $98.7\pm 1.1$  & $96.4\pm 2.9$  & $97.2\pm 2.2$ & $97.0\pm 3.3$ & 11 & 12& 13 & 13  \\ 
  \midrule                                                                                                                                                          
  GroundSeg3D\quad{}  & $94.8\pm 5.1$ & $94.8\pm 4.4$ & $97.5\pm 2.1$ & $92.8\pm 4.8$ & $86.9\pm 11.5$ & $78.4\pm 14.8$ & $86.7\pm 14.2$ & $88.8\pm 5.7$ & 85& 81& 98 & \!\!102\\ 
  \bottomrule
\end{tabular}%
\end{table*}
\else
\begin{table}[tb]
  \centering
  \caption{Comparison of ground segmentation on SemanticKITTI sequence 00 with vegetation}
  \label{tab:comparison_00_non_downsampled}
  \begin{tabular}{@{}ll@{\ \ }l@{\ \ }l@{\ \ }r@{}}
  \toprule
   Algorithm   & Precision$\pm$SD (\%) & Recall$\pm$SD (\%) & F1 (\%) & Time (ms)  \\
\midrule
  GPF          & $94.95\pm 4.99$ & $79.02\pm27.20$ & 86.26 & 13 \\ 
  R-GPF        & $59.20\pm 11.56$& $96.57\pm 1.83$ & 73.40 & 18 \\ 
  GPR          & $81.03\pm7.50$  & $95.94\pm2.54$  & 87.86 & 11 \\ 
  CascadedSeg  & $91.65\pm 9$    & $71.00\pm10.4$  & 80.00 & 65 \\ 
  RANSAC       & $87.52\pm 14.63$& $93.35\pm11.38$ & 90.84 & 64  \\ 
  LineFit      & $98.15\pm 0.97$ & $83.05\pm 9.94$ & 89.97 & 7 \\ 
  Patchwork    & $92.34\pm 3.31$ & $94.61\pm 3.44$ & 93.46 & 17 \\ 
  Patchwork++  & $94.62\pm 2.76$ & $98.73\pm 1.10$ & 96.62 & 11  \\ 
  \bottomrule
  GroundSeg3D  & $94.79\pm 5.05$ & $86.92\pm 11.47$ & 90.69 & 85\\ 
  \bottomrule
\end{tabular}%
\end{table}

\begin{table}[tb]
  \centering
  \caption{Comparison of ground segmentation on SemanticKITTI sequence 01 with vegetation}
  \label{tab:comparison_01_non_downsampled}
  \begin{tabular}{@{}ll@{\ \ }l@{\ \ }l@{\ \ }r@{}}
  \toprule
   Algorithm   & Precision$\pm$SD (\%) & Recall$\pm$SD (\%) & F1 (\%) & Time (ms)  \\
\midrule
  GPF          & $92.06\pm 8.53$ & $23.52\pm21.15$ & 37.47 & 11 \\ 
  R-GPF        & $89.71\pm 6.05$ & $89.70\pm 4.65$ & 89.70 & 18 \\ 
  GPR          & $92.46\pm4.37$  & $89.58\pm4.93$  & 91    & 12 \\ 
  CascadedSeg  & $97.10\pm 2.91$ & $73.44\pm10.30$ & 83.63 & 64 \\ 
  RANSAC       & $96.46\pm 2.36$ & $96.00\pm3.77$  & 96.20 & 64  \\ 
  LineFit      & $99.00\pm 1.52$ & $74.22\pm 9.46$ & 74.30 & 6 \\ 
  Patchwork    & $96.00\pm 3.53$ & $89.10\pm 5.23$ & 92.36 & 16 \\ 
  Patchwork++  & $98.40\pm 0.72$ & $96.40\pm 2.92$ & 97.40 & 12  \\ 
  \bottomrule
  GroundSeg3D  & $94.81\pm 4.35$ & $78.35\pm 14.79$ & 85.80& 81\\ 
  \bottomrule
\end{tabular}%
\end{table}

\begin{table}[tb]
  \centering
  \caption{Comparison of ground segmentation on SemanticKITTI sequence 04 with vegetation}
  \label{tab:comparison_04_non_downsampled}
  \begin{tabular}{@{}ll@{\ \ }l@{\ \ }l@{\ \ }r@{}}
  \toprule
   Algorithm   & Precision$\pm$SD (\%) & Recall$\pm$SD (\%) & F1 (\%) & Time (ms)  \\
\midrule
  GPF          & $95.42\pm 5.02$ & $72.55\pm28.70$ & 82.43 & 12 \\ 
  R-GPF        & $84.56\pm 9.00$ & $93.43\pm 1.72$ & 88.77 & 18 \\ 
  GPR          & $94.98\pm 2.00$ & $94.41\pm1.54$  & 94.19 & 13 \\ 
  CascadedSeg  & $98.20\pm 2.42$ & $72.50\pm3.72$  & 83.41 & 70 \\ 
  RANSAC       & $96.00\pm 2.04$ & $97.00\pm1.34$  & 96.15 & 42  \\ 
  LineFit      & $99.51\pm 0.51$ & $80.10\pm 4.64$ & 88.74 & 6.5 \\ 
  Patchwork    & $97.43\pm 1.40$ & $91.25\pm 2.83$ & 94.24 & 17 \\ 
  Patchwork++  & $98.15\pm 0.80$ & $97.18\pm 2.21$ & 97.66 & 13  \\ 
  \bottomrule
  GroundSeg3D  & $97.46\pm 2.14$ & $86.65\pm 14.18$ & 91.74& 98\\ 
  \bottomrule
\end{tabular}%
\end{table}

\begin{table}[tb]
  \centering
  \caption{Comparison of ground segmentation on SemanticKITTI sequence 09 with vegetation}
  \label{tab:comparison_09_non_downsampled}
  \begin{tabular}{@{}ll@{\ \ }l@{\ \ }l@{\ \ }r@{}}
  \toprule
   Algorithm   & Precision$\pm$SD (\%) & Recall$\pm$SD (\%) & F1 (\%) & Time (ms)  \\
\midrule
  GPF          & $96.00\pm 6.20$ & $63.50\pm29.53$ & 76.36 & 12 \\ 
  R-GPF        & $71.36\pm 12.12$& $95.35\pm 3.36$ & 81.63 & 19 \\ 
  GPR          & $00.00\pm 0.0$  & $00.00\pm0.0$  & 0.0    & 0.0 \\ 
  CascadedSeg  & $94.80\pm 5.80$ & $70.14\pm8.00$ & 81     & 84 \\ 
  RANSAC       & $93.26\pm 7.82$ & $89.27\pm6.20$ & 91.22  & 64  \\ 
  LineFit      & $98.00\pm 1.27$ & $80.33\pm 9.00$ & 88.25 & 6  \\ 
  Patchwork    & $92.74\pm 3.26$ & $92.15\pm 4.57$ & 92.44 & 17 \\ 
  Patchwork++  & $95.40\pm 3.40$ & $97.00\pm 3.32$ & 96.14 & 13  \\ 
  \bottomrule
  GroundSeg3D  & $92.84\pm 4.79$ & $88.76\pm 5.71$ & 90.75 & 102\\ 
  \bottomrule
\end{tabular}%
\end{table}
\fi
\subsection{Limitations}

Figures \ref{fig:gaps} to~\ref{fig:combined} show under- and over-segmentation due to gaps and some vegetation points.
These can cause the precision to drop below 94\% (Table~\ref{tab:comparison_non_downsampled}, Seq.\@~09).

\begin{figure}[tb]
  \centering
  \includegraphics[width=\linewidth]{figures/limitations/gaps.png}
  \caption{Under-segmentation of ground points due to point cloud with gaps exceeding grid cell size}
  \label{fig:gaps}
\end{figure}

\begin{figure}[tb]
  \centering
  \includegraphics[width=\linewidth]{figures/limitations/vegetation.png}
  \caption{Over-segmentation of some vegetation points}
  \label{fig:vegetation}
\end{figure}

\begin{figure}[tb]
  \centering
  \includegraphics[width=\linewidth]{figures/limitations/combined.png}
  \caption{Over-segmentation of some vegetation points and exclusion of ground points after a large gap}
  \label{fig:combined}
\end{figure}

\section{Acknowledgment}
Parts of the algorithm were written with support of ChatGPT.
%\todo{Proudly presented by \ldots{} But funding should be mentioned in footnote on first page}
\iffalse

The preferred spelling of the word ``acknowledgment'' in America is without 
an ``e'' after the ``g''. Avoid the stilted expression ``one of us (R. B. 
G.) thanks $\ldots$''. Instead, try ``R. B. G. thanks$\ldots$''. Put sponsor 
acknowledgments in the unnumbered footnote on the first page.

\fi

\bibliography{paper}


\end{document}
