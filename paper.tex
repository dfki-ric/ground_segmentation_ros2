\pdfminorversion=4
\pdfcompresslevel=9
\pdfobjcompresslevel=2
\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{subcaption} % For subfigures
\usepackage{textcomp}
\usepackage{xcolor}
% \def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
%     T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

%\usepackage[textsize=tiny]{todonotes}
\usepackage{multirow}
\usepackage{booktabs}

\usepackage{gensymb}
\usepackage{enumitem}
\usepackage{float}

\bibliographystyle{IEEEtran}

\graphicspath{{figures}}


\newenvironment{smallcases}{\left\{\begin{smallmatrix}}{\end{smallmatrix}\right.}


\begin{document}

\title{GroundSeg3D: A Robust 3D Grid-Based Hybrid Approach for Ground Segmentation in Point Clouds Across Varied Terrains}

\author{Muhammad Haider Khan Lodhi and Christoph Hertzberg% <-this % stops a space
\thanks{Both authors are researchers at the Robotics Innovation Center, 
German Research Center for Artificial Intelligence (DFKI),
Bremen, Germany.
Email: {\tt\small Haider\_Khan.Lodhi@dfki.de, Christoph.Hertzberg@dfki.de}}
\thanks{Funded by the German Federal Ministry of Education and Research, grant number: 13N16537.
All code will be published after acceptance.}%
% \thanks{$^{1}$Albert Author is with Faculty of Electrical Engineering, Mathematics and Computer Science,
%         University of Twente, 7500 AE Enschede, The Netherlands
%         {\tt\small albert.author@papercept.net}}%
% \thanks{$^{2}$Bernard D. Researcheris with the Department of Electrical Engineering, Wright State University,
%         Dayton, OH 45435, USA
%         {\tt\small b.d.researcher@ieee.org}
}%
\iffalse
\author{\IEEEauthorblockN{Muhammad Haider Khan Lodhi and Christoph Hertzberg}
\IEEEauthorblockA{\textit{Robotics Innovation Center}, 
German Research Center for Artificial Intelligence (DFKI),
Bremen, Germany\\
Email: Haider\_Khan.Lodhi@dfki.de, Christoph.Hertzberg@dfki.de}%
}%
\fi

\maketitle

\begin{abstract}
Ground segmentation in point cloud data is the process of separating the points
into ground and non-ground points. It is a key component in the perception pipeline of
an autonomous system. The state-of-the-art solutions for ground segmentation are often designed for use-cases from
urban environments. The performance of such solutions is tightly linked to certain assumptions about the environment and the
sensor configuration. Additionally, they require hand tuning of parameters. There is a need to have a robust,
multi-terrain capable ground segmentation solution which does not require tuning of parameters. In this work, we provide such a solution for
indoor and outdoor mobile robotics and autonomous driving applications. Our solution
combines the strengths of different approaches as a single solution and produces
good ground segmentation of point cloud data on varied terrains.
\end{abstract}

% \begin{IEEEkeywords}
% Ground Segmentation, Point Clouds, Mobile Robotics, Autonomous Driving, Rough Terrain, LiDAR, 3D~Grid-Based Approach
% \end{IEEEkeywords}


\section{Introduction}
The perception system of an autonomous system utilizes multiple sensors to
capture salient features of the environment. These features are used as inputs by various subsystems, e.\,g., navigation, control, learning etc. A core sensor of the perception system
is the LiDAR and given the advancements in sensor design and reduced costs, it
has become a standard sensor in autonomous systems \cite{Gomes2023}. The LiDAR uses bands of
laser beams to scan the surrounding environment and the time-of-flight
information is used to capture distance information at high resolution. The
result of the single scan is a point cloud sample which in today’s sensors may
contain more than a million points. Evidently, the point cloud captures the
points from the environment in the scanning region of interest. However, not all 
points in the region of interest are relevant for the downstream algorithms. 
%For example, the obstacle detection 
%algorithm is only interested in the obstacle points and the road detection algorithm only requires the ground points. 
For example, object and obstacle detection algorithms often detect ground points as false
    positives and removal of ground points improves performance and reduces computational burden \cite{Firkat2023}.
%
Additionally, ground points can be used for traversability analysis, navigation, and
    static map generation which makes ground segmentation a necessary precursor step for extraction
    of useful information from point cloud data in autonomous systems \cite{TraversabilitySurvey, tsaiground, Arora2023,Jimenez2021,Quatro}.
\begin{figure}[b]
  \centering
  \includegraphics[width=0.7\linewidth]{intro_combined.png}
  \caption{GroundSeg3D: The point cloud is segmented into ground points (magenta) and non-ground points (blue). }
  \label{fig:results}
\end{figure}

\iffalse
Feeding the same point cloud sample to both algorithms would require redudant processing of irrelevant points in both 
algorithms. To mitigate the redudancy, it is a usual practice to segment the points into ground and non-ground points. 
The segmentation task is performed by a ground segmentation algorithm and it has many applications:
\begin{itemize}
  \item Ground Detection
  \item Obstacle Detection
  \item Object Detection
  \item Terrain Analysis
  \item Traversability Analysis
  \item Drivable Area Detection
  \item SLAM
\end{itemize}

Ground point extraction from a point cloud sample is a crucial step in the perception pipeline of autonomous systems. 
Numerous solutions have been proposed over the past few years. 
\fi

In this work, our primary focus is to extract ground 
points with minimal parameter dependency, eliminating the need for parameter tuning, and ensure consistent performance across 
varied terrains and sensor configurations. Our algorithm was tested on real-world datasets, featuring different 
types of point cloud sensors in various configurations and environments. 

\iffalse
\subsection{Benefits of Ground Segmentation}
\begin{itemize}
  \item Detection of ground points is a necessary precursor step for extraction
    of useful information from point cloud data for downstream tasks in mobile
    robot navigation \cite{Jimenez2021,Arora2023}.
  \item Object and obstacle detection algorithms detect ground points as false
    positives. The detection can be improved when ground points are detected and
    removed. Additionally, the computational burden is reduced by processing only
    non-ground points \cite{Firkat2023}.
  \item Ground points can be used for traversability analysis, navigation, and
    static map generation.
\end{itemize}
\fi

\section{Related Work}
Existing ground segmentation methods typically rely on either traditional techniques or learning-based approaches \cite{Gomes2023}. 

\subsection{Traditional Solutions}
The traditional solutions are usually based 
on occupancy grids or elevation maps \cite{5650541,9558794,GroundGrid}. Some
approaches extend the grid cell and analyze the points in neighboring cells for
a better estimation of surface properties \cite{9969541}.
The modeling of the points in the
grid cells using Gaussian process regression, plane fitting and line extraction is a common approach in many
solutions \cite{MultilevelGS,GPR,linefit,R-GPF,RANSAC,CascadedSeg,9466396,9981561,StagedGS,Hy-Seg}. Region growing and clustering is used to combine individual grid
cells into a larger region of connected cells \cite{5164280,GPF}. Higher order inferences based
on Conditional Random Field (CRF) and Markov Random Field (MRF) have also been used
\cite{9410344,7995861}. 

\subsection{Learning Solutions}
The advancements in deep learning meant that some researchers used the existing
CNN networks originally used for object detection for ground segmentation using
range images generated from the point cloud data. The development of PointNet \cite{qi2017pointnet} and its successor 
PointNet++ \cite{qi2017pointnet++} gave a unified
solution for various applications, e.\,g., object and 
scene segmentation. Solutions like GATA \cite{GraphBasedGS}, GndNet \cite{paigwar2020gndnet}, PointPillars \cite{lang2019pointpillars}, JCP \cite{rs13163239}, SectorGSnet \cite{9691325}
show good results.

\section{Proposed Solution}
\iffalse
Our goal was to develop a ground segmentation solution based
on the traditional approaches and after analyzing the state-of-the-art, we decided
to combine different approaches as a single solution by utilizing the best of
all approach.

\subsection{Key Challenges in the Existing Traditional Solutions}
We noticed some key shortfalls in the existing solutions and aimed to come up with a solution which can solve the open challenges.
Below we list some of the key challenges. The challenges were found out after careful experimentation of various datasets and sensor 
configurations. We documented the challenges in the experimentaion section at the end of the paper.

The key challenges faced when working with the existing state-of-the-art solutions are that they;
\begin{itemize}
  \item require fine tuning of many algorithm parameters for best performance.
  \item have high degree of performance dependency on the type of sensor used for the point cloud.
  \item are developed for the autonomous driving scenario with only a few solutions showcasing adequate performance
        on rough uneven terrains.
  \item show good performance on the SemanticKITTI datset but do not adapt to other datasets from the autonomous driving domain.
\end{itemize}

Below we discuss some some key challenges which researches try to solve in the
ground segmentation approaches and suggest our selected solution for it.

\subsection{Computational Requirements}
A high-resolution LiDAR sensor can generate millions of data points. For
instance, the Velodyne VLS-128 can produce up to 9.6 M points per second in the
dual-return mode, with frame rates varying from 5 Hz to 10 Hz. In a typical
operation, this sensor can be configured to produce, on average, a point cloud
of 2,403,840 points per second (240,384 points at 10 Hz), which makes it harder
to analyze the entire point cloud in real time during the navigation tasks.

\paragraph*{Solution} We use a grid-based technique in 3D space to help mitigate these problems, 
where each grid cell contain its assigned points and can be processed individually on demand.

\subsection{Terrain Roughness}
The assumption of flat ground surfaces may be plausible in a urban environment but it 
does not always hold. Autonomous systems usually have to navigate 
on uneven terrain with varying degrees of roughness. Therefore, the ground segementation algorithms
are not always easily applicable for msuch systems.

\paragraph*{Solution} We use a combination of local and global terrain properties, e.\,g., surface gradiant,
local spacial distribution of points, and global connectivity. We apply local eigen analysis to extract local 
terrain surface properties and utilize a connectivity-based region growing algorithm based on initial seed selection. 

\subsection{Size of Grid Cell}
We faced the challenge regarding the selection of a suitable grid cell size in 
relation to the roughness of the terrain. In principle, small grid cells are better 
suited for model fitting but the grid cell's size has a direct impact on the computational performance
of the algorithm. The selected size also depends on the gap of the scan lines.
The scan lines in close proximity to the robot are dense and the distance
between the lines increases based on the distance from the robot. Irrespective
of the grid cell size, a cell close to the robot is more likely to have points
from multiple scan lines assigned to it. This means that such cells are better
suited to fit a plane model. As a consequence, it is possible to have an
accurate slope estimate of the local terrain surface. On the contrary, cells which
are assigned points far away from the robot are more likely to have only single
scan lines assigned to them. A plane fitting algorithm can fit a plane to the
line of points but the slope estimate is highly uncertain and is usually
unreliable. Additionally to this, a small cell size is more sensitive to noisy 
sensor data and has high computational costs.

Furthermore, the size of the grid cells is also closely related to the type of
application and environment. In indoor environments with flat surfaces, it is
generally unproblamatic to have large grid cell size as compared to outdoor
uneven terrain. To sum up, there is so one size fits all solution when it comes
to selection of a suitable grid cell size for a ground segmentation algorithm.

\paragraph*{Solution} We make use a phase-wise approach and vary the size of grid
cell. In the first phase, we use a large height for the grid cells to capture as 
many tall structures as possible. The second Phase-Is applied on the resultant 
ground points of the first phase and we use a small height for the grid cells to 
remove any false positive ground points.

\subsection{Point Sparsity}
The point assignment to grid cells is non-uniform and
depends on various factors. It is not feasible to fit a plane to all
grid cells because some cells may have a single line of points or a random
distribution of points. This is especially the case when using a fixed grid size
and the distance of the cell from the point cloud origin increases.

\paragraph*{Solution} We perform local eigen analysis on each grid cell. The eigen values
encode a good representation of the spacial distribution of the points assigned to the cell. 
Afterwards, we classify each grid cell into one of the three classes: Line, Planar, Non-Planar. Each 
cell type is handed with a different technique most appropritate for it. 

\subsection{Polar \& Square Coordinates}
Literature review has shown that researchers prefer polar grid cells as compared
to square cells. Various types of polar grid cells have been researched.
\paragraph*{Solution} However, in our experiments we found a square grid cell
with a fixed grid size performs better than polar cells. The reason in our view
is that as the distance from the sensor origin increases, the cells get bigger
and as a consequence, results in the deterioration of the performance of the
plane fitting based segmentation.

\subsection{False positive flat surfaces, e.\,g., table top, car roof etc.}
The slope of the fitting plane to a grid cell can not be used as the sole
criteria for the segmentation. The reason is the existence of flat surfaced
obstacles in the robot's environment.
\paragraph*{Solution} We apply region growing based on high confidence intial
ground seed cells. The connectivity-based expansion ensures that only
neighboring ground cells are expanded.

\subsection{False positive points at the junction of ground and non-ground points}
The results of the segmentation depend on the handling of junctions between
ground and non-ground points. This is where the majority of cases of under and over
segmentation occur. 

\paragraph*{Solution} A point-wise inlier check utilizing KDTree’s is used at the
junction of ground and non-ground cells to detect and correct over and under segmentation.

\subsection{Influence of grid cell height on the performance of segmentation}
The height of the grid cell plays a key role in identification of non-ground points in
the point cloud. The key challenge here is to detect tall non-ground objects in the grid
cells but without false detection the underlying ground points as non-ground points.
This challenge is evident in the tree canopy situation where the ground points
in the grid cell are falsely identified as non-ground points because of the large
height of the grid cell.
\paragraph*{Solution} The point space is sub-divided into 3D grid cells for a clear distinction
between ground and non-ground points. Additionally, we use the same strategy as mentioned in the 
point G to detect and correct segmentation mistakes.

\subsection{Fitted Plane Outlier Correction}
The real world ground surface has a curvature and plane fitting usually results in false 
positive outliers upon fitting a ideal flat surface to the the points. 
\paragraph*{Solution} It is common practice to define a distance threshold to fit a planar model to a set of 
points. We also use a fixed distance threshold for the initial plane fit for for slope estimation. However, the 
extraction of ground and non-ground points uses an additional correction step using the actual resultant normal of 
the points assinged to the cell. We use the strategy as mentioned in the point G to detect and correct segmentation mistakes.

\subsection{Performance}
The requirement for real-time operation of the ground segmentation is very
evident for the application in the autonomous driving community. Therefore,
solutions need to able to do a highly accurate segmentation within in stipulated
time period. The exact time requirements may vary based on the industry.

\paragraph*{Solution} \todo{Time and space complexity analysis needs to be done}

\section{Core Components}
\fi

GroundSeg3D takes a single point cloud and outputs the 
ground and non-ground points. 
%In this section, we explain the key processing steps performed on each input for robust ground segmentation.
%
\iffalse
\subsection{Data Preparation and Filtering}
The input point cloud is cropped to a user-defined region of interest, which varies depending on the application. 
For autonomous driving, a larger region of interest is preferred, while mobile robotics typically require a smaller
to medium-sized region.
\fi
%\subsection{Two-Phase Segmentation of Points Based on Local Surface Properties and Neighborhood Analysis}
%
The segmentation process is divided into two distinct phases (Fig.\@~\ref{fig:phases}).
%each phase uses grid cells of varying heights to improve the accuracy of ground and non-ground detection, 
%while also managing potential false segmentation as shown in Fig.\@~\ref{fig:dual_phase_segmentation}.
\begin{figure}[tb]
  \centering
  \includegraphics[width=0.8\linewidth]{figures/overview/stages.png}
  \caption{Phase-I uses grid cells with large height
  to capture tall structures as non-ground points. The coarse ground points from Phase-I are subsequently processed in Phase-II using grid cells with small height.}
  \label{fig:phases}
\end{figure}
%
In the first phase of segmentation, a large height for the grid cells is 
used to capture as many non-ground structures as possible within each grid cell. However, 
the increased cell height can lead to false segmentation of ground points, 
especially where the ground is obscured by overhead objects. 
These false detections are addressed in subsequent stages of the algorithm.

\begin{figure}[tb]
  \centering
  \includegraphics[width=0.8\linewidth,trim={0 0 0 1.5cm},clip]{figures/corrections/dual_phase_segmentation.png}
  \caption{Correction of over-segmentation of ground points based on dual-phase segmentation.}
  \label{fig:dual_phase_segmentation}
\end{figure}

The second phase uses a smaller grid cell height to 
refine the segmentation results from the first phase. 
In this phase, false positive ground points from Phase-I are identified as obstacle points and false positive 
non-ground points are identified as ground points based on neighborhood correction (Fig.\@~\ref{fig:dual_phase_segmentation}).

\iffalse
\begin{figure}[tb]
  \centering
  \includegraphics[width=0.8\linewidth]{figures/overview/pipeline.png}
  \caption{The core stages in the pipeline of GroundSeg3D. The same steps in the pipeline are used in both phases with slight modifications.}
  \label{fig:pipeline}
\end{figure}
\fi
Table~\ref{tab:phase_behavior} shows the variation in the processing pipelines of in the two phases.
The steps are described in the following sections.

\begin{table}[tb]
  \centering
  \caption{Behavior of steps in the first and second phase of segmentation}
  \label{tab:phase_behavior}
%  \todo[inline]{Only write this inside text}
  \begin{tabular}{@{}p{3.3cm}p{1.9cm}@{\quad}p{2.9cm}@{}}
\toprule
  \textbf{Step} & \textbf{First Phase} & \textbf{Second Phase} \\
\midrule
  \textbf{Grid Representation} & Grid cells have a large height & Grid cells have a small height \\
  \textbf{Local Eigen Analysis} & Unchanged & Unchanged \\
  \textbf{Surface Gradient Analysis} & Unchanged & Unchanged \\
  \textbf{Ground Region Growth} & All 17 neighbors below and at the same level are expanded & A detailed neighborhood check is performed for all 26 neighboring cells during expansion \\
  \textbf{Final Ground Segmentation\!\!\!} & Unchanged & Unchanged \\
\bottomrule
  \end{tabular}%
\end{table}

\subsection{Grid Representation}
The points are assigned to cells within a 3D grid 
to model and detect empty spaces. 
\begin{align}
cell_{\{x,y,z\}} &= \lfloor point_{\{x,y,z\}} / cellsize_{\{x,y,z\}}\rfloor,
%cell_y &= point_y / cellsize_y \\
%cell_z &= point_z / cellsize_z
\end{align}
where $cellsize$ is a parameter. 
%We explain our choice of parameters in section \ref{sec:parameters}.

\subsection{Local Eigen Analysis}

\begin{figure}
  \centering
\begin{subcaptiongroup}
  \includegraphics[width=0.8\linewidth,trim={0 1cm 0 0},clip]{celltypes.png}
  \phantomcaption\label{fig:LineCell}
  \phantomcaption\label{fig:PlaneCell}
  \phantomcaption\label{fig:UnknownCell}
\end{subcaptiongroup}
%   \subcaptionbox{Line Cell\label{fig:LineCell}}[0.3\linewidth]{
%   \includegraphics[width=\linewidth]{line_cell.png}
%   }
%   \hfil
%   \subcaptionbox{Plane Cell\label{fig:PlaneCell}}[0.3\linewidth]{
%   \includegraphics[width=\linewidth]{plane_cell.png}
%   }
%   \hfil
%   \subcaptionbox{Non-Planar Cell\label{fig:UnknownCell}}[0.3\linewidth]{
%   \includegraphics[width=\linewidth]{unknown_cell.png}
%   }
  \caption{Points (black) assigned to a grid cell and the 
  dominating eigen vectors (green) are shown. 
  The cell is classified as Line~(\subref{fig:LineCell}), Planar~(\subref{fig:PlaneCell}),
  or Non-Planar~(\subref{fig:UnknownCell})
  based of the local eigen analysis.}
\end{figure}
We use local eigen 
analysis to estimate the local spatial distribution of points within each grid cell.
For each grid cell, we calculate the eigenvalues of the covariance matrix of the points.
Let $\lambda_1 > \lambda_2 > \lambda_3$ be the eigenvalues, we then compute the ratio:
%
%We use the ratio of the largest eigenvalue to the sum of all eigenvalues to gauge the point distribution:
%
\begin{align}
  \text{Ratio} &= \tfrac{\lambda_1}{\lambda_1 + \lambda_2 + \lambda_3}
\end{align}  
%
The ratio of the largest eigenvalue to the sum of all eigenvalues provides an indication of whether the points in 
a grid cell are more aligned with a line, a planar surface, or a non-planar structure.

%TODO make sure it's in remaining text
\iffalse
\begin{description}%[style=nextline]
  \item[Line Cell]
  A high ratio in range $[0.9, 1.0]$ indicates that $\lambda_1$ is significantly larger than $\lambda_2$ and $\lambda_3$, which suggests that the points 
  are distributed along a line. In this case, the distribution is highly elongated along one direction with minimal variation
  in the other directions.
%
  \item[Planar Cell]
  A ratio in this range $[0.4, 0.9)$ indicates that $\lambda_1$ is moderately larger than $\lambda_2$ and $\lambda_3$, suggesting that the points 
  are more aligned with a planar surface.
%
  \item[Non-planar Cell]
  A ratio in this range $[0.0, 0.4)$ indicates that $\lambda_1$ is relatively smaller than $\lambda_2$ and $\lambda_3$, suggesting that the points 
  are more aligned with a non-planar surface.
\end{description}
\fi
\iffalse
\begin{table}
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Cell Type} & \textbf{Threshold} \\
\midrule
Line Cell      & $> 0.95$ \\
Plane Cell     & $\ge 0.40$ \\
Non-planar Cell   & $< 0.40$ \\
\bottomrule
\end{tabular}
\caption{Largest eigen to the sum ratio}
\label{tab:eigen-ratio}
\end{table}
\fi

%Each type of cell is processed using a distinct algorithm, and we refer to these cells as Line, Plane, and Non-Planar cells:

\subsubsection{Line Cell}\label{sec:LineCell}



If the largest eigenvalue is significantly larger than the other eigenvalues, the cell is categorized as a Line Cell.
For these cells, we compute the angle between the largest eigenvector 
(depicted in green in Fig.~\ref{fig:LineCell}) and the $z$-axis.
%
\begin{align}
CellType &= 
  \begin{smallcases}
    \text{Obstacle}         & angle \ge 90\degree - Threshold \\
    \text{Tentative Ground} & angle < 90\degree - Threshold
  \end{smallcases}\label{eq:Line:GroundThreshold}
\end{align}
%
where $Threshold$ refers to the ground slope threshold.

The ground slope threshold is a parameter and is used to determine the classification of cells based on their angle of uprightness. 
% Cells with a small angle between the largest eigenvector and the positive z-axis are considered for classification
% as obstacles. This is because ground cells typically do not exhibit a highly upright spatial distribution.
For tentative ground cells we cannot be certain of their exact nature.  
As a result, such cells are marked
as tentative ground until it is clarified in the region growth step (Section~\ref{sec:region_growth}).

\subsubsection{Planar Cell}
% \begin{figure}
%   \centering
%   \includegraphics[width=\linewidth]{plane_cell.png}
%   \caption{Points (white) assigned to a grid cell and the corresponding eigen vectors (black) are shown. 
%   Two dominant eigen vectors and shown and the third vector is negligible in comparison. 
%   The cell is classified as Plane based on the local eigen analysis.}
%   \label{fig:PlaneCell}
% \end{figure}
Cells that contain multiple scan lines are suitable candidates for plane fitting when the spatial distribution of the points 
exhibits two dominant eigenvalues, with the third eigenvalue being negligibly small or significantly smaller. The planar cells at 
this stage can not be considered as definitive ground. It might be the case that the cell
exhibiting a planar distrubtion of points is a car roof top or the top of an obstacle. 
%\todo{Suggestion: One could check lowest EV/EV sum $<$ threshold}

\subsubsection{Non-planar Cell}
% \begin{figure}
%   \centering
%   \includegraphics[width=\linewidth]{unknown_cell.png}
%   \caption{Points (white) assigned to a grid cell and the corresponding eigen vectors (black) are shown. 
%   There is no dominance of a single or two eigen vectors. }
%   \label{fig:UnknownCell}
% \end{figure}

If a cell has no dominant eigenvalue, the cell is classified as non-planar, suggesting that the points do not align well with a planar surface.

\subsection{Surface Gradient Analysis}
\begin{figure}
  \centering
  \includegraphics[width=0.5\linewidth]{plane_model_fit.png}
  \caption{Plane model fit for local surface slope estimation.}
  \label{fig:PlaneFit}
\end{figure}

For planar cells,
RANSAC plane fitting is applied with a given threshold,
getting a plane and separating the points into inliers and outliers.
We compute the slope of 
the fitted plane and apply a slope threshold to assess whether the cell meets the ground criteria. The resultant ground cells 
are used as candidates for initial seeds in the region-growing algorithm.
%
\begin{align}
CellType &= 
  \begin{smallcases} 
    \text{Tentative Ground}, & angle \le Threshold \\
    \text{Non-Ground}, & angle > Threshold
  \end{smallcases} \label{eq:Planar:Ground}
\end{align}
where $Threshold$ refers to the same ground slope threshold used in (Section~\ref{sec:LineCell}, equation~\ref{eq:Line:GroundThreshold}).

\subsection{Ground Region Growth}\label{sec:region_growth}

The ground region growth selects seeds from tentative ground cells and recursively expands them until 
all cells are expanded based on neighborhood connectivity.

%\subsubsection{Seed Selection} \todo{starting paragraph rephrase}
The seed cells are selected from the tentative ground cells based on the following conditions:

\begin{enumerate}
  \item High Confidence of Ground Classification: The cell is high confidence when a large percentage of total points 
  are inliers of the planar model fit.
  
  \item Proximity to Sensor: Cells that are far 
  from the sensor typically have sparse points with large gaps, making them unsuitable for initial seed selection due to limited 
  connectivity with neighboring cells. Therefore, cells in close proximity to the sensor are selected.
  
  \item Quadrant-Based Seed Selection: Seed cells are selected from each of the four quadrants in the Euclidean space. 
  This approach helps mitigate issues related to large occlusions or gaps in the point cloud, which could inhibit region growth. 
%   By selecting seed cells from four distinct groups (one per quadrant), we improve the 
%   coverage and connectivity of the region-growing process.
   
  \item Selection of a Single Seed Cell: From each valid group, one seed cell is selected based on the following criteria:
  \begin{itemize}
  \item The cell must have the highest number of tentative ground neighbors compared to other cells in the group.
  \item It should be closest to the mean height of the group.
  \end{itemize}
  
  \item Final Seed Cells: After shortlisting seed cells in all quadrants, we select the lowest cell from the front (1st and 2nd quadrants) 
  and the back (3rd and 4th quadrants) in GroundSeg3D and use all four seed cells in all quadrants for GroundSeg3D+ as shown in Table~\ref{tab:comparison_non_downsampled}. The seed cells are then used for region growing.
  \end{enumerate}

In Phase-I we recursively expand all ground neighbors of the seed cells in the grid. 
This region-wise growth process ensures that only actual ground cells are identified from the set of tentative ground cells.

In Phase-II we only expand cells if they are close to each other along the cell normal.
This ensures continuity of the resulting ground cells.
%The second cell is expanded if most of its points are within the ground-inliner threshold along the cell normal of the first.


% Phase-II includes an additional step in which points from neighboring cells above or below the expanded cell are checked. If most points in a neighboring cell 
% are within the ground inlier threshold, that cell is selected for further expansion. Conversely, if the majority of a neighboring cell's points are not 
% within the ground inlier threshold, the cell is not expanded. This step prevents expanding cells that meet the slope threshold criteria but are actually obstacles.


\subsection{Final Ground Segmentation}

The final ground segmentation refines the ground and obstacle cells by correcting over- and under-segmentation.

\subsubsection{Ground Points}

\begin{figure}[tb]
  \centering
  \includegraphics[width=0.8\linewidth]{figures/corrections/false_outliers/overview_1.png}
  \caption{False outliers (non-ground points) from RANSAC plane fitting on uneven terrain.}
  \label{fig:false_outliers1}
\end{figure}

\iffalse
\begin{figure}[tb]
  \centering
  \includegraphics[width=0.8\linewidth]{figures/corrections/false_outliers/overview_2.png}
  \caption{Overview2}
  \label{fig:false_outliers2}
\end{figure}
\fi

\begin{figure}[tb]
  \centering
  \includegraphics[width=0.8\linewidth]{figures/corrections/false_outliers/overview_3.png}
  \caption{Correction of false outliers using eigen normal (eigen vector of the smallest eigen value) instead of plane normal.}
  \label{fig:false_outliers3}
\end{figure}

As plane fitting on rough terrain produces false outliers, we iterate through the outliers of each ground cell, use KDTree to
find the closest inlier point and compute the distance along the eigen normal direction (Fig.\@~\ref{fig:false_outliers3}).
If the distance is below the ground threshold, the point is considered a ground point.

% As shown in Fig.\@~\ref{fig:false_outliers1}, plane fitting to planar cells leads to false 
% outliers because the fitted plane does not accurately model the spatial distribution of the 
% points in the cell. Therefore we use the eigen normal to correct these points (Fig.\@~\ref{fig:false_outliers3}).
% 
% 
% 
% Each ground cell obtained from the region-growing Phase-Is processed individually. 
% This helps us determine whether any outliers from the plane fitting were incorrectl y marked. Specifically, we use the smallest 
% eigenvalue of the points in the cell, rather than the normal to the plane fit, to check for false negatives. This pointwise check uses KDTree's and
% helps in identifying and correcting false outliers.
% 
% The process involves the following steps:
% 
% \paragraph*{Outlier Check}
% 
% \begin{enumerate}
% \item Identify the closest inlier point and compute the vector from this inlier to the outlier.
% \item Calculate the component of this vector along the ground normal.
% \item Based on the distance along the normal:
% \begin{enumerate}
% \item If the distance is less than or equal to the plane fit threshold, the outlier is classified as a ground point.
% \item If the distance exceeds the plane fit threshold, the outlier is classified as a non-ground point.
% \end{enumerate}
% \end{enumerate}

\subsubsection{Non-Ground Cells}

%\subsection{Neighborhood Correction}

\begin{figure}[tb]
  \centering
  \includegraphics[width=0.8\linewidth,trim={0 0 0 1.5cm},clip]{figures/corrections/neighborhood_correction.png}
  \caption{Correction of under-segmentation of ground points based on neighboring cells. False detections are marked in white circles.}
  \label{fig:neighborhood_correction}
\end{figure}


Non-Ground cells often contain some ground cells, especially at boundaries of obstacles and ground surfaces.
Similar to outliner check for ground points, we check every obstacle point, against the closest ground points in neighboring cells.

\iffalse
To address these challenges, we perform the following checks:
\begin{enumerate}
\item Neighbor Check: We first check if a non-ground cell has any ground neighbors. This step is crucial because false segmentation 
often occurs at the junctions between obstacles and ground cells.
\begin{itemize}
\item If the non-ground cell has no ground neighbors, all its points are classified as non-ground.
\item If the non-ground cell does have ground neighbors, we need to perform further analysis to determine the validity of these neighbors.
\end{itemize}
\item Pointwise Check: For non-ground cells with ground neighbors, we perform a pointwise comparison. This involves analyzing the 
obstacle points in the cell and comparing them with neighboring ground points. The goal is to identify and remove any false positive obstacle points.
\end{enumerate}

By implementing these procedures, we refine the segmentation of non-ground cells and improve the overall accuracy of the point 
classification. This approach helps in reducing false positives and ensuring that the point cloud data accurately reflects the 
true nature of the environment.
\fi

\section{Experiments}

A ground segmentation algorithm must give robust and reliable performance without parameter tuning on varied terrains. The performance must be consistent across 
various types of
point cloud sensors. 
For comparison, we selected state-of-the-art
ground segmentation algorithms mentioned in Table~\ref{tab:comparison_non_downsampled} using their default parameters. 

Some of the ground segmentation algorithms require 
the sensor height as a parameter. We believe that a constant sensor height can not be guaranteed for point cloud data in real world applications, e.\,g., handheld sensor,
 drones, legged robots 
which can change their height etc. On the contrary, GroundSeg3D does not depend on the sensor height. Furthermore, the state-of-the-art algorithms require extensive 
hand-tuning of many parameters while
GroundSeg3D does not require any parameter tuning.

For all experiments with GroundSeg3D, we use grids of sizes $(2,2,10)$ and $(2,2,0.5)$ meters in Phase-I and \hbox{Phase-II} respectively. The ground inlier threshold is 0.1\,m 
and the ground slope threshold is 30\textdegree.

%combined table
\begin{table*}[tb]
  \centering
  \caption{Comparison of ground segmentation on SemanticKITTI sequences 00, 01, 04, and 09 with vegetation. As explained in section \ref{sec:region_growth}, two and four seed cells are used for expansion in GroundSeg3D and GroundSeg3D+ respectively.}.
  \label{tab:comparison_non_downsampled}
  \begin{tabular}{@{}l@{} l@{\ \ }l@{\ \ }l@{\ \ }l  l@{\ \ }l@{\ \ }l@{\ \ }l   r@{\ \ }r@{\ \ }r@{\ \ }r}
  \toprule
   Algorithm   & \multicolumn{4}{c}{Average precision $\pm$ standard deviation (\%)} & \multicolumn{4}{c}{Average recall $\pm$ standard deviation (\%)} & \multicolumn{4}{c}{Avg. Time (ms)}  \\
   \cmidrule(lr){2-5} \cmidrule(lr){6-9} \cmidrule(lr){10-13}
   \multicolumn{1}{r}{Sequence:}& 00 & 01 & 04 & 09 & 00 & 01 & 04 & 09 & 00  & 01 & 04 & 09\\
\midrule
  GPF \cite{GPF}         & $94.9\pm 5.0$ & $92.1\pm 8.5$ & $95.4\pm 5.0$ & $96.0\pm 6.2$ & $79.0\pm27.2$  & $23.5\pm21.2$  & $72.6\pm28.7$ & $63.5\pm29.5$ & 13 & 11& 12 & 12 \\ 
  R-GPF \cite{R-GPF}       & $59.2\pm 11.6$& $89.7\pm 6.0$ & $84.6\pm 9.0$ & $71.4\pm 12.1$& $96.6\pm 1.8$  & $89.7\pm 4.6$  & $93.4\pm 1.7$ & $95.4\pm 3.4$ & 18 & 18& 18 & 19 \\ 
  GPR \cite{GPR}         & $81.0\pm 7.5$ & $92.5\pm 4.5$ & $95.0\pm 2.0$ & $84.8\pm 7.0$ & $95.9\pm2.5$   & $89.6\pm4.9$   & $94.4\pm1.5$  & $95.0\pm3.1$  & 11 & 12& 13 & 13\\ 
  CascadedSeg \cite{CascadedSeg} & $91.7\pm 9.0$ & $97.1\pm 2.9$ & $98.2\pm 2.4$ & $94.8\pm 5.8$ & $71.0\pm10.4$  & $73.4\pm10.3$  & $72.5\pm3.7$  & $70.1\pm8.0$  & 65 & 64& 70 & 84 \\ 
  RANSAC \cite{RANSAC}      & $87.5\pm 14.6$& $96.5\pm 2.5$ & $96.0\pm 2.0$ & $93.3\pm 7.8$ & $93.3\pm11.4$  & $96.0\pm3.8$   & $97.0\pm1.3$  & $89.3\pm6.2$  & 64 & 64& 42 & 64  \\ 
  LineFit \cite{linefit}     & $98.1\pm 1.0$ & $99.0\pm 1.5$ & $99.5\pm 0.5$ & $98.0\pm 1.3$ & $83.1\pm 9.9$  & $74.2\pm 9.5$  & $80.1\pm 4.6$ & $80.3\pm 9.0$ & 7  & 6 & 6 & 6  \\ 
  Patchwork \cite{9466396}   & $92.3\pm 3.3$ & $96.0\pm 3.5$ & $97.4\pm 1.4$ & $92.7\pm 3.3$ & $94.6\pm 3.4$  & $89.1\pm 5.2$  & $91.2\pm 2.8$ & $92.2\pm 4.6$ & 17 & 16& 17 & 17 \\ 
  Patchwork++ \cite{9981561}\quad{} & $94.6\pm 2.8$ & $98.4\pm 0.7$ & $98.2\pm 0.8$ & $95.4\pm 3.4$ & $98.7\pm 1.1$  & $96.4\pm 2.9$  & $97.2\pm 2.2$ & $97.0\pm 3.3$ & 11 & 12& 13 & 13  \\ 
  \midrule                                                                                                                                                          
  GroundSeg3D\quad{}  & $94.8\pm 5.1$ & $94.8\pm 4.4$ & $97.5\pm 2.1$ & $92.8\pm 4.8$ & $86.9\pm 11.5$ & $78.4\pm 14.8$ & $86.7\pm 14.2$ & $88.8\pm 5.7$ & 85& 81& 98 & \!\!102\\ 
  GroundSeg3D+\quad{}  & $94.4\pm 5.0$ & $94.4\pm 4.5$ & $97.8\pm 1.6$ & $92.2\pm 5.2$ & $89.1\pm 6.3$ & $80.4\pm 6.8$ & $89.0\pm 2.6$ & $89.3\pm 4.3$ & 84& 80& 98 & \!\!102\\ 
  \bottomrule
\end{tabular}%
\end{table*}



\subsection{Datasets}
We selected datasets from urban and natural environments. Each dataset uses a different point cloud sensor. The platforms used for data collection are Car, Mobile Robot and Handheld.

\begin{table}[tb]
  \centering
  \caption{Varied datasets from natural and urban environments including average runtime of GroundSeg3D for the first 500 frames from a single sequence}
  \label{tab:datasets}
  \begin{tabular}{@{}p{2.2cm}p{1.1cm}@{}p{1.3cm}@{}p{0.8cm}@{}p{1.4cm}@{}p{1.0cm}@{}p{0.7cm}@{}}
  \toprule
  \textbf{Dataset} & \parbox[b]{1.1cm}{\textbf{Environ-ment}} & \textbf{Sensor} & \textbf{Year} & \textbf{Platform} & \parbox[b]{1.0cm}{\textbf{Sensor Height}} & \parbox[b]{0.7cm}{\textbf{Time (ms)}}\\
  \midrule
  {SemanticKITTI\,\cite{Geiger2013IJRR}\!\!\!\!\!} & Urban   & HDL-64E & 2013 & Car         & 1.72\,m & 86 \\ 
  {UrbanNav\,\cite{Hsunavi.602}}                   & Urban   & HDL-32E & 2021 & Car         & 2.1\,m  & 42 \\ 
  {DOALS\,\cite{9560730}}                          & Urban   & OS0 128 & 2021 & Handheld    & 2.0\,m  & 60 \\ 
  {RELLIS-3D\,\cite{9561251}}                      & Natural & OS1 64  & 2022 & Mob.\,Robot & 1.16\,m & 82 \\
  {BotanicGarden\,\cite{10415477}\!\!}             & Natural & VLP-16  & 2024 & Mob.\,Robot & 1.16\,m & 21 \\
  \bottomrule
  \end{tabular}%
\end{table}
\iffalse
\subsubsection{SemanticKITTI}
The SemanticKITTI dataset \cite{Geiger2013IJRR} is a comprehensive collection of real-world data designed to
facilitate research in autonomous driving and computer vision. Collected by the
Karlsruhe Institute of Technology and the Toyota Technological Institute at
Chicago, it includes data captured from a vehicle equipped with various sensors,
such as high-resolution stereo cameras, 3D laser scanners, and GPS/IMU systems.
This dataset covers a wide range of urban, suburban, and rural environments,
providing benchmarks for tasks such as object detection, tracking, segmentation,
and visual odometry. Its diversity and complexity make it a pivotal resource for
advancing autonomous driving technologies and robust computer vision algorithms.

\subsubsection{UrbanNav}
The UrbanNav dataset, available on GitHub, is a comprehensive dataset designed for urban navigation research, 
particularly in Global Navigation Satellite System (GNSS)-denied environments. Developed by the Intelligent 
Positioning and Navigation Laboratory at The Hong Kong Polytechnic University, it provides data collected 
from various sensors, including GNSS receivers, Inertial Measurement Units (IMUs), Light Detection and Ranging (LiDAR) sensors, 
and cameras, in dense urban settings like Hong Kong. The dataset is intended to facilitate the development and testing of robust 
navigation algorithms that can handle the challenges of urban environments, such as multipath effects and signal blockages. The 
repository includes detailed descriptions of the data collection process, file structure, and usage guidelines, making it a valuable 
resource for researchers working on urban navigation and related fields.

\subsubsection{DOALS}
Dynamic Object Aware LiDAR SLAM based on Automatic Generation of Training Data (DOALS) contains 12000 scans that were recorded
in the main hall of ETH Zurich. The recording was performed at various locations and each location provides 2 sequences of data.
We use the data collection from the touristic pedestrian zone (Nierendorf) for our ground segmentation tests.

\subsubsection{BotanicGarden}
The BotanicGarden dataset is a used to test the ground segmentation capabilities of the algorithm in unstructured natual environments. The
dataset provides data from wide varierty of sensors, including high-res and high-rate stereo gray and RGB cameras, 
spinning and MEMS 3D LiDARs, and low-cost and industrial-grade IMUs, supporting a wide range of applications. 

\subsubsection{RELLIS-3D}
The Rellis-3D dataset, developed by Texas A\&M University, is an advanced dataset
aimed at improving semantic segmentation in outdoor environments. Named after
the RELLIS Campus, it features high-resolution LiDAR and camera data captured in
diverse weather conditions and various terrain types. Unlike many other datasets
focused on urban settings, Rellis-3D emphasizes off-road scenarios, including
complex and cluttered scenes with vegetation, rough terrain, and obstacles. This
dataset is instrumental for developing and testing algorithms in fields such as
robotics, autonomous navigation, and environmental perception, where
understanding complex 3D structures and diverse landscapes is crucial.
\fi
\subsection{Performance Metrics}

We use precision and recall to compare the algorithms.
\begin{align}
\text{Precision} &= \tfrac{\text{NTP}}{\text{NTP} + \text{NFP}}, &
\text{Recall} &= \tfrac{\text{NTP}}{\text{NTP} + \text{NFN}},
%\text{F1} &= 2 \cdot \tfrac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}},
\end{align}
where
NTP, NFP, and NFN are the Number of True Positives (correctly predicted positives), 
False Positives (incorrectly predicted positives)
and False Negatives (positives that were incorrectly predicted as negative).

Not all datasets provided labelled point clouds, therefore the performance metrics were 
collected only on the SemanticKITTI dataset. Visual analysis of ground segmentation results on other datasets was enough to compare the performance of GroundSeg3D and
state-of-the-art solutions.

\iffalse
\subsection{GroundSeg3D Parameters}\label{sec:parameters}
One of our main objectives was to develop an algorithm which does not require parameter tuning across various environments and sensor configurations. We achieved our target and
all the experimental results from various datasets use a single set of parameters. 

\begin{table}[tb]
  \centering
  \caption{Parameters of GroundSeg3D}
  \label{tab:parameters}
  \begin{tabular}{@{}p{3cm}p{1cm}p{4cm}@{}}
\toprule
  \textbf{Parameter} & \textbf{Value} & \textbf{Description} \\
\midrule
  \textbf{max\{X,Y,Z\}, min\{X,Y,Z\}} & $\pm80$ & Region of interest (in meters) of the point cloud in Euclidean space. \\ 
  \textbf{downsample} & False & Toggle to downsample the point cloud to a user-defined resolution in meters. \\ 
  \textbf{downsample\_resolution} & 0.1 & Downsample point cloud to 10\,cm. \\ 
  \textbf{(cellSizeX, cellSizeY, cellSizeZ)} & 2, 2, 10 & Size (in meters) of the grid cell in three-dimensional space. \\ 
  \textbf{startCellDistanceThreshold} & 10 & Maximum distance for selection of seed cells used as start cells for region growing. \\
  \textbf{groundInlierThreshold} & 0.1 & Inlier threshold (in meters) for the plane model fitting to points in a grid cell. \\ 
\bottomrule
  \end{tabular}%
  \label{tab:parameters}
\end{table}
\fi
\section{Results and Discussion}

Figures \ref{fig:single_fig} to~\ref{fig:botanic_garden} show consistent performance of GroundSeg3D in different datasets in natural and urban environments with different sensor configurations. 
We also successfully tested the algorithm on our own Robotics Test Track with very rough terrain (Fig.\@~\ref{fig:arter_all}).

\begin{figure}[tb]
  \centering
  \includegraphics[width=0.8\linewidth,trim={0 0 0 1.5cm},clip]{figures/grouped_figures/kitti_all.png}
  \caption{GroundSeg3D results on the SemanticKITTI dataset.}
  \label{fig:single_fig}
\end{figure}

\begin{figure}[tb]
  \centering
  \includegraphics[width=0.8\linewidth]{figures/sloped terrain/kitti_all.png}
  \caption{GroundSeg3D results on sloped outdoor terrain in SemanticKITTI with color-coded height.}
  \label{fig:kitti_sloped_terrain}
\end{figure}


\begin{figure}[tb]
  \centering
  \includegraphics[width=0.8\linewidth,trim={0 0 0 1.5cm},clip]{figures/grouped_figures/urban_nav_all.png}
  \caption{GroundSeg3D results on the UrbanNav Tokyo dataset.}
  \label{fig:urban_nav}
\end{figure}

\begin{figure}[tb]
  \centering
  \includegraphics[width=0.8\linewidth,trim={0 0 0 1.5cm},clip]{figures/grouped_figures/doals_all.png}
  \caption{GroundSeg3D results on the DOALS dataset.}
  \label{fig:doals}
\end{figure}


\begin{figure}[tb]
  \centering
  \includegraphics[width=0.8\linewidth,trim={0 0 0 1.5cm},clip]{figures/grouped_figures/rellies3d_all.png}
  \caption{GroundSeg3D results on the RELLIS-3D dataset.}
  \label{fig:rellies_3d}
\end{figure}

\begin{figure}[tb]
  \centering
  \includegraphics[width=0.8\linewidth,trim={0 0 0 1.5cm},clip]{figures/grouped_figures/botanic_garden_all.png}
  \caption{GroundSeg3D results on the BotanicGarden dataset.}
  \label{fig:botanic_garden}
\end{figure}

\iffalse
\begin{figure}[tb]
  \centering
  \includegraphics[width=\linewidth]{figures/dfki_track/actual_backyard.png}
  \caption{Rough outdoor terrain at the Robotics Test Track on DFKI RIC}
  \label{fig:actual_backyard}
\end{figure}
\fi
\begin{figure}[tb]
  \centering
  \includegraphics[width=0.7\linewidth]{figures/dfki_track/mfh.png}
  \includegraphics[width=0.8\linewidth,trim={0 3.5cm 0 5.5cm},clip]{figures/dfki_track/actual_backyard.png}
  \includegraphics[width=0.8\linewidth,trim={0 0 0 0cm},clip]{figures/sloped terrain/arter_all.png}
  \caption{GroundSeg3D results on rough indoor and outdoor terrain at the Robotics Test Track at DFKI RIC}
  \label{fig:arter_all}
\end{figure}


On SemanticKITTI sequences 00, 01, 04, and~09 the average precision of GroundSeg3D is generally above~94\%, but due to some false positive ground points from vegetation (Fig.\@~\ref{fig:vegetation}), the precision drops to~92\% in sequence~09.
The average recall stays above 86\%, except in sequence~01, due to gaps in the point cloud larger than grid cell size (Fig.\@~\ref{fig:gaps}). Increasing the number of seeds sells to 4 improves recall with significantly reduced standard deviation as shown in Table~\ref{tab:comparison_non_downsampled}.
\begin{figure}[tb]
  \centering
  \includegraphics[width=0.8\linewidth,trim={0 3cm 0 0cm},clip]{figures/limitations/vegetation.png}
  \caption{Over-segmentation of some vegetation points.}
  \label{fig:vegetation}
\end{figure}

\begin{figure}[tb]
  \centering
  \includegraphics[width=0.8\linewidth]{figures/limitations/gaps.png}
  \caption{Under-segmentation of ground points due to point cloud with gaps exceeding grid cell size.}
  \label{fig:gaps}
\end{figure}

\iffalse
\begin{figure}[tb]
  \centering
  \includegraphics[width=\linewidth]{figures/limitations/combined.png}
  \caption{Over-segmentation of some vegetation points and exclusion of ground points after a large gap}
  \label{fig:combined}
\end{figure}
\fi


While average precision and recall give a good estimate of performance, we can see in sequence 00 frame 20, 100, 140, 150 (Fig.\@ \ref{fig:group_frame_20} to~\ref{fig:group_frame_150})
that GroundSeg3D shows consistent performance.

\begin{figure}[tb]
  \centering
  \includegraphics[width=\linewidth,trim={0 0 0 1.5cm},clip]{figures/metrics/group_frame_20.png}
  \caption{Frame 20 of the 0th sequence in the SemanticKITTI.}
  \label{fig:group_frame_20}
\end{figure}

\begin{figure}[tb]
  \centering
  \includegraphics[width=\linewidth,trim={0 0 0 1.5cm},clip]{figures/metrics/group_frame_100.png}
  \caption{Frame 100 of the 0th sequence in the SemanticKITTI. Notice the significantly greater number of false positives [blue] points in the
  Patchwork and Patchwork++ solutions.}
  \label{fig:group_frame_100}
\end{figure}

\begin{figure}[tb]
  \centering
  \includegraphics[width=\linewidth,trim={0 0 0 1.5cm},clip]{figures/metrics/group_frame_140.png}
  \caption{Frame 140 of the 0th sequence in the SemanticKITTI.}
  \label{fig:group_frame_140}
\end{figure}

\begin{figure}[tb]
  \centering
  \includegraphics[width=\linewidth,trim={0 0 0 1.5cm},clip]{figures/metrics/group_frame_150.png}
  \caption{Frame 150 of the 0th sequence in the SemanticKITTI.}
  \label{fig:group_frame_150}
\end{figure}


\iffalse
\begin{figure}[tb]
  \centering
  \includegraphics[width=\linewidth,trim={0 0 0 1.5cm},clip]{figures/grouped_figures/ric.png}
  \caption{Ground segmentation results from data collected at DFKI RIC - MFH}
  \label{fig:ric}
\end{figure}
\fi

GroundSeg3D is slower in runtime but
in terms of average precision and recall we are on-par with other state-of-the-art algorithms on SemanticKITTI (Table~\ref{tab:comparison_non_downsampled})
but outperform them on other datasets (Fig.\@ \ref{fig:comparison_botanic_garden} and~\ref{fig:comparison_tokio_urban}).

\begin{figure}[tb]
  \centering
  \includegraphics[width=\linewidth]{comparison_botanic_garden.png}
  \caption{Comparison on the Botanic Garden data set. 
  Green circles highlight false obstacles, yellow circles highlight false ground points.
  Here CascadedSeg fails to detect most ground points and R-GPF fails to detect many non-ground points.}
  \label{fig:comparison_botanic_garden}
\end{figure}

\begin{figure}[tb]
  \centering
  \includegraphics[width=\linewidth]{comparison_tokio_urban.png}
  \caption{Comparison on the Tokio UrbanNav data set. GPF fails to detect any ground point, R-GPF fails to detect many non-ground points. All algorithms at least make minor errors.}
  \label{fig:comparison_tokio_urban}
\end{figure}



%We further analyze the runtime of GroundSeg3D on various datasets (Last column in Table~\ref{tab:datasets}).

\iffalse
\begin{table}[tb]
  \centering
  \caption{Runtime of GroundSeg3D on varied datasets}
  \label{tab:runtime}
\begin{tabular}{@{}lr@{}}
\toprule
  \textbf{Dataset} & \textbf{Average Time (ms)} \\
\midrule
  SemanticKITTI & 86 \\ 
  UrbanNav      & 42 \\ 
  DOALS         & 60 \\ 
  RELLIS-3D     & 82 \\
  BotanicGarden & 21 \\
\bottomrule
  \end{tabular}%
\end{table}
\fi

%\subsection{Dual-Phase Ground Segmentation}

%\subsection{Performance on Rough Terrain}


%\subsection{Performance on Sloped Terrain}


%\subsection{Performance on Rocky and Flat Terrains}

\iffalse
%\subsection{GroundSeg3D Performance vs. Distance}

In this section, we discuss the experimentation results. We take the SemanticKITTI sequence 0 to analyze the performance metrics with the state-of-the-art solutions. We wanted 
to see how the performance metrics are affected as the distance of points increases. We can see from the plots that the precision is least affected by increasing
point distance. The recall is slighly reduced with increases distances and this is due to the region growth step in our solution. As the distance between the scan lines
inceases, the far off points beloging to scan lines with gaps greator than the cell size are ignored in the region growth. The reduced recall scores are specially evident
in the Sequence 1 because of gaps in point cloud samples due to occlusions and separatation between the two streets. We believe that although the reduced recall is compensated
by the capability of GroundSeg3D to exhibit robust performance on varied terrains without parameter tuning and dependence to a particular sensor configuration.

\begin{figure}[tb]
  \centering
  \begin{subfigure}[b]{0.5\textwidth}
      \centering
      \includegraphics[width=\textwidth]{figures/graphs/00.jpg}
      \label{fig:subfig1}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.5\textwidth}
      \centering
      \includegraphics[width=\textwidth]{figures/graphs/01.jpg}
      \label{fig:subfig2}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.5\textwidth}
      \centering
      \includegraphics[width=\textwidth]{figures/graphs/04.jpg}
      \label{fig:subfig3}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.5\textwidth}
      \centering
      \includegraphics[width=\textwidth]{figures/graphs/09.jpg}
      \label{fig:subfig4}
  \end{subfigure}
  \caption{Performance metrics of GroundSeg3D vs. Distance}
  \label{fig:main_fig}
\end{figure}
\fi

%\subsection {Correction of False Outliers}






\iffalse
\begin{figure*}
  \centering
  \includegraphics[width=16cm, height=8cm]{figures/datasets/UrbanNav/combined.png}
  \caption{Comparison with state-of-the-art solutions on UrbanNav Tokyo Dataset}
  \label{fig:urbannav_combined}
\end{figure*}

\begin{figure*}
  \centering
  \includegraphics[width=16cm, height=8cm]{figures/datasets/BotanicGarden/combined.png}
  \caption{Comparison with state-of-the-art solutions on BotanicGarden Dataset}
  \label{fig:botanicgarden_combined}
\end{figure*}
\fi

\iftrue
\else
\begin{table}[tb]
  \centering
  \caption{Comparison of ground segmentation on SemanticKITTI sequence 00 with vegetation}
  \label{tab:comparison_00_non_downsampled}
  \begin{tabular}{@{}ll@{\ \ }l@{\ \ }l@{\ \ }r@{}}
  \toprule
   Algorithm   & Precision$\pm$SD (\%) & Recall$\pm$SD (\%) & F1 (\%) & Time (ms)  \\
\midrule
  GPF          & $94.95\pm 4.99$ & $79.02\pm27.20$ & 86.26 & 13 \\ 
  R-GPF        & $59.20\pm 11.56$& $96.57\pm 1.83$ & 73.40 & 18 \\ 
  GPR          & $81.03\pm7.50$  & $95.94\pm2.54$  & 87.86 & 11 \\ 
  CascadedSeg  & $91.65\pm 9$    & $71.00\pm10.4$  & 80.00 & 65 \\ 
  RANSAC       & $87.52\pm 14.63$& $93.35\pm11.38$ & 90.84 & 64  \\ 
  LineFit      & $98.15\pm 0.97$ & $83.05\pm 9.94$ & 89.97 & 7 \\ 
  Patchwork    & $92.34\pm 3.31$ & $94.61\pm 3.44$ & 93.46 & 17 \\ 
  Patchwork++  & $94.62\pm 2.76$ & $98.73\pm 1.10$ & 96.62 & 11  \\ 
  \bottomrule
  GroundSeg3D  & $94.79\pm 5.05$ & $86.92\pm 11.47$ & 90.69 & 85\\ 
  \bottomrule
\end{tabular}%
\end{table}

\begin{table}[tb]
  \centering
  \caption{Comparison of ground segmentation on SemanticKITTI sequence 01 with vegetation}
  \label{tab:comparison_01_non_downsampled}
  \begin{tabular}{@{}ll@{\ \ }l@{\ \ }l@{\ \ }r@{}}
  \toprule
   Algorithm   & Precision$\pm$SD (\%) & Recall$\pm$SD (\%) & F1 (\%) & Time (ms)  \\
\midrule
  GPF          & $92.06\pm 8.53$ & $23.52\pm21.15$ & 37.47 & 11 \\ 
  R-GPF        & $89.71\pm 6.05$ & $89.70\pm 4.65$ & 89.70 & 18 \\ 
  GPR          & $92.46\pm4.37$  & $89.58\pm4.93$  & 91    & 12 \\ 
  CascadedSeg  & $97.10\pm 2.91$ & $73.44\pm10.30$ & 83.63 & 64 \\ 
  RANSAC       & $96.46\pm 2.36$ & $96.00\pm3.77$  & 96.20 & 64  \\ 
  LineFit      & $99.00\pm 1.52$ & $74.22\pm 9.46$ & 74.30 & 6 \\ 
  Patchwork    & $96.00\pm 3.53$ & $89.10\pm 5.23$ & 92.36 & 16 \\ 
  Patchwork++  & $98.40\pm 0.72$ & $96.40\pm 2.92$ & 97.40 & 12  \\ 
  \bottomrule
  GroundSeg3D  & $94.81\pm 4.35$ & $78.35\pm 14.79$ & 85.80& 81\\ 
  \bottomrule
\end{tabular}%
\end{table}

\begin{table}[tb]
  \centering
  \caption{Comparison of ground segmentation on SemanticKITTI sequence 04 with vegetation}
  \label{tab:comparison_04_non_downsampled}
  \begin{tabular}{@{}ll@{\ \ }l@{\ \ }l@{\ \ }r@{}}
  \toprule
   Algorithm   & Precision$\pm$SD (\%) & Recall$\pm$SD (\%) & F1 (\%) & Time (ms)  \\
\midrule
  GPF          & $95.42\pm 5.02$ & $72.55\pm28.70$ & 82.43 & 12 \\ 
  R-GPF        & $84.56\pm 9.00$ & $93.43\pm 1.72$ & 88.77 & 18 \\ 
  GPR          & $94.98\pm 2.00$ & $94.41\pm1.54$  & 94.19 & 13 \\ 
  CascadedSeg  & $98.20\pm 2.42$ & $72.50\pm3.72$  & 83.41 & 70 \\ 
  RANSAC       & $96.00\pm 2.04$ & $97.00\pm1.34$  & 96.15 & 42  \\ 
  LineFit      & $99.51\pm 0.51$ & $80.10\pm 4.64$ & 88.74 & 6.5 \\ 
  Patchwork    & $97.43\pm 1.40$ & $91.25\pm 2.83$ & 94.24 & 17 \\ 
  Patchwork++  & $98.15\pm 0.80$ & $97.18\pm 2.21$ & 97.66 & 13  \\ 
  \bottomrule
  GroundSeg3D  & $97.46\pm 2.14$ & $86.65\pm 14.18$ & 91.74& 98\\ 
  \bottomrule
\end{tabular}%
\end{table}

\begin{table}[tb]
  \centering
  \caption{Comparison of ground segmentation on SemanticKITTI sequence 09 with vegetation}
  \label{tab:comparison_09_non_downsampled}
  \begin{tabular}{@{}ll@{\ \ }l@{\ \ }l@{\ \ }r@{}}
  \toprule
   Algorithm   & Precision$\pm$SD (\%) & Recall$\pm$SD (\%) & F1 (\%) & Time (ms)  \\
\midrule
  GPF          & $96.00\pm 6.20$ & $63.50\pm29.53$ & 76.36 & 12 \\ 
  R-GPF        & $71.36\pm 12.12$& $95.35\pm 3.36$ & 81.63 & 19 \\ 
  GPR          & $00.00\pm 0.0$  & $00.00\pm0.0$  & 0.0    & 0.0 \\ 
  CascadedSeg  & $94.80\pm 5.80$ & $70.14\pm8.00$ & 81     & 84 \\ 
  RANSAC       & $93.26\pm 7.82$ & $89.27\pm6.20$ & 91.22  & 64  \\ 
  LineFit      & $98.00\pm 1.27$ & $80.33\pm 9.00$ & 88.25 & 6  \\ 
  Patchwork    & $92.74\pm 3.26$ & $92.15\pm 4.57$ & 92.44 & 17 \\ 
  Patchwork++  & $95.40\pm 3.40$ & $97.00\pm 3.32$ & 96.14 & 13  \\ 
  \bottomrule
  GroundSeg3D  & $92.84\pm 4.79$ & $88.76\pm 5.71$ & 90.75 & 102\\ 
  \bottomrule
\end{tabular}%
\end{table}
\fi

\section{Conclusions}

This paper introduces a hybrid multi-phase ground segmentation algorithm for 3D point clouds, designed to operate without 
the need for parameter tuning or dependence on specific sensor configurations, ensuring ease of deployment. Extensive experimental 
evaluations demonstrate that the proposed GroundSeg3D algorithm consistently delivers robust performance across diverse and 
challenging natural and urban environments.

In future research, we plan to improve the runtime and explore new ways for selection of the seed cells.

\section{Acknowledgment}
We greatly acknowledge the open source Ground Segmentation Benchmark \cite{9466396,9981561} which we used for benchmarking. PCL \cite{PCL} and Nanoflann \cite{NANOFLANN} were used for 
processing the point clouds
%\footnote{https://github.com/url-kaist/Ground-Segmentation-Benchmark}
and parts of our algorithm were written with support of ChatGPT \cite{openai2023chatgpt}.

\clearpage

%\todo{Proudly presented by \ldots{} But funding should be mentioned in footnote on first page}
\iffalse

The preferred spelling of the word ``acknowledgment'' in America is without 
an ``e'' after the ``g''. Avoid the stilted expression ``one of us (R. B. 
G.) thanks $\ldots$''. Instead, try ``R. B. G. thanks$\ldots$''. Put sponsor 
acknowledgments in the unnumbered footnote on the first page.

\fi

\bibliography{paper}


\end{document}
